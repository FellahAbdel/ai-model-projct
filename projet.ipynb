{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Exo 1 - Préparation des données \n",
    "\n",
    "# importation des données\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "# import du fichier\n",
    "data = pd.read_csv(\"synthetic.csv\")\n",
    "\n",
    "# Visualisation des données\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Nombre de colonnes (attributs) dans le DataFrame\n",
    "num_attributes = data.shape[1]\n",
    "\n",
    "# Afficher le nombre d'attributs\n",
    "print(f\"Le nombre d'attributs dans le fichier est : {num_attributes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type de données et valeurs manquantes\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoir le nombre d'attributs dans le modèle\n",
    "print(data.columns)\n",
    "# 14 attributs dans le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les classes uniques dans la colonne 'Class'\n",
    "classes_uniques = data['Class'].unique()\n",
    "\n",
    "# Nombre de classes différentes\n",
    "num_classes = len(classes_uniques)\n",
    "\n",
    "# Afficher le nombre de classes différentes\n",
    "print(f\"Le nombre de classes différentes dans les données est : {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combien d'instances compte chaque classe?\n",
    "nbr_instances = data['Class'].value_counts()\n",
    "print(nbr_instances)\n",
    "\n",
    "# Sortie \n",
    "# Class\n",
    "# 1    908\n",
    "# 0    674\n",
    "# 2    472\n",
    "# 3    244\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les données sont-elles linéairement séparables ?\n",
    "Non, si on observe le schéma 1 on voit que les données ne le sont pas.\n",
    "De plus si l'on choisit de les ranger par classe , on peut s'apercevoir que \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # import biblio matplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['Attr_A'], data['Attr_B'], c=data['Class'], alpha=0.5, cmap='viridis')\n",
    "plt.xlabel('Attribut 1')\n",
    "plt.ylabel('Attribut 2')\n",
    "plt.title('Scatter Plot des attributs par classe')\n",
    "plt.colorbar(label='Classe')\n",
    "plt.show()\n",
    "\n",
    "# On peut voir clairement que ce n'est pas divisible linéairement à l'état brut\n",
    "# je pense que use image est vraiment mieux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 et 6 (voir compte-rendu.md) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Mise en oeuvre des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir un attribut à analyser, par exemple 'Attr_A'\n",
    "attribute = 'Attr_A'\n",
    "\n",
    "\n",
    "# Calculer les quartiles pour l'attribut choisi\n",
    "quartiles = data[attribute].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# Sort the attribute values and print them\n",
    "sorted_attribute = data[attribute].sort_values()\n",
    "print(sorted_attribute)\n",
    "print(quartiles)\n",
    "# Afficher les quartiles\n",
    "print(f\"Quartile 1 (Q1) de l'attribut '{attribute}': {quartiles[0.25]}\")\n",
    "print(f\"Médiane (Q2) de l'attribut '{attribute}': {quartiles[0.5]}\")\n",
    "print(f\"Quartile 3 (Q3) de l'attribut '{attribute}': {quartiles[0.75]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Mise en oeuvre des modèles.\n",
    "\n",
    "# Arbre de décision \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbre de décision\n",
    "\n",
    "# Calcul de l'entropie\n",
    "\n",
    "\"\"\"\n",
    "L'entropie est une mesure de l'incertitude associée à une variable aléatoire.\n",
    "\"\"\"\n",
    "\n",
    "def entropie(dataframe , attribut_cible):  \n",
    "    # Calcul de la probabilité de chaque classe\n",
    "    compte_classe = dataframe[attribut_cible].value_counts()\n",
    "    #print(compte_classe)\n",
    "    proba = compte_classe / compte_classe.sum()\n",
    "    #print(proba) \n",
    "    # Calcul de l'entropie\n",
    "    entropie = - (proba * np.log2(proba+ np.finfo(float).eps)).sum() # éviter log2(0)\n",
    "    return entropie\n",
    "\n",
    "# Test de la fonction\n",
    "print(entropie(data, 'Attr_A'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "1.8608867211835993\n",
    "1.860886721183598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour calculer tous les quartiles d'un attribut donné\n",
    "def calculate_quartiles(data, attribute):\n",
    "    return data[attribute].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# Test de la fonction sur le DataFrame chargé\n",
    "\n",
    "print(calculate_quartiles(data, 'Attr_A'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"Attr_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()\n",
    "sorted = data.sort_values(by=\"Attr_A\")\n",
    "print(len(sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_information(dataframe, attribut_cible, attribut_test):\n",
    "    \"\"\"\n",
    "    Calculate the information gain from splitting the data based on a test attribute.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data to partition.\n",
    "    attribut_cible (str): The target attribute we want to predict.\n",
    "    attribut_test (str): The attribute whose gain we want to calculate.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - attribut_test (str): The test attribute.\n",
    "        - max_gain (float): The maximum information gain obtained.\n",
    "        - best_split_value (float): The split value that provides the best gain.\n",
    "        - best_partitions (tuple): A tuple containing two DataFrames representing the lower and upper partitions\n",
    "          resulting from the best split.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial entropy of the target attribute\n",
    "    entropie_initiale = entropie(dataframe, attribut_cible)\n",
    "\n",
    "    # The gain, split_value and partitions initialized\n",
    "    max_gain = 0\n",
    "    best_split_value = None\n",
    "    best_partitions = None\n",
    "\n",
    "    # Check for no unique values in the attribute being tested\n",
    "    if len(dataframe[attribut_test].unique()) <= 1:\n",
    "        return None\n",
    "\n",
    "    # Sorting data by the attribute to test\n",
    "    sorted_data = dataframe.sort_values(by=attribut_test)\n",
    "\n",
    "    # Unique values of the attribute to test, considering quartiles to reduce complexity\n",
    "    quartiles = calculate_quartiles(sorted_data, attribut_test).to_list()\n",
    "\n",
    "    # Adding the min and max values to cover the entire range of the attribute\n",
    "    quartiles = [sorted_data[attribut_test].min()] + quartiles + \\\n",
    "        [sorted_data[attribut_test].max()]\n",
    "    # Voir si je n'enlève pas min et max valeur\n",
    "\n",
    "    # Iterating through the sorted unique values to find the best split\n",
    "    for split_value in quartiles:\n",
    "        # Partitioning the data based on the split value\n",
    "        lower_partition = sorted_data[sorted_data[attribut_test] < split_value]\n",
    "        upper_partition = sorted_data[sorted_data[attribut_test]\n",
    "                                      >= split_value]\n",
    "\n",
    "        # Calculating the weighted entropy for the partitions\n",
    "        # Row counts.\n",
    "        total_instances = len(sorted_data)\n",
    "        lower_weight = len(lower_partition) / total_instances\n",
    "        upper_weight = len(upper_partition) / total_instances\n",
    "\n",
    "        # Computing the weighted_entropy\n",
    "        weighted_entropy = (lower_weight * entropie(lower_partition, attribut_cible)) + \\\n",
    "                           (upper_weight * entropie(upper_partition, attribut_cible))\n",
    "\n",
    "        # Information gain for the current split\n",
    "        current_gain = entropie_initiale - weighted_entropy\n",
    "\n",
    "        # If the current gain is greater than the max_gain, update max_gain and best_split_value\n",
    "        if current_gain > max_gain:\n",
    "            max_gain = current_gain\n",
    "            best_split_value = split_value\n",
    "            best_partitions = (lower_partition, upper_partition)\n",
    "\n",
    "    # Returning the attribute, gain, split_value, and partitions as a tuple\n",
    "    return attribut_test, max_gain, best_split_value, best_partitions\n",
    "\n",
    "\n",
    "# Testing the function with an example attribute\n",
    "# Let's use 'Attr_A' as the attribute to test and 'Class' as the target\n",
    "test_gain_info = gain_information(data, 'Class', 'Attr_H')\n",
    "test_gain_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_gain(dataframe, attribut_cible):\n",
    "    \"\"\"\n",
    "    Calculate the best information gain and corresponding split in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The input data as a pandas DataFrame.\n",
    "    attribut_cible (str): The target attribute that we want to predict (e.g. 'Class').\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the following elements:\n",
    "        - best_attribute (str): The attribute that yields the best information gain.\n",
    "        - best_gain (float): The highest information gain observed.\n",
    "        - best_split_value (float): The split value that produces the best gain.\n",
    "        - best_partitions (tuple): A tuple containing the two partitions resulting from the best split.\n",
    "    \"\"\"\n",
    "    # Initialize variables to track the best gain and the associated attribute\n",
    "    best_gain = 0\n",
    "    best_attribute = None\n",
    "    best_split_value = None\n",
    "    best_partitions = None\n",
    "\n",
    "    # Iterate over all the attributes in the DataFrame, except the target attribute\n",
    "    for test_attribute in dataframe.columns:\n",
    "        if test_attribute == attribut_cible:\n",
    "            continue  # Skip the target attribute\n",
    "\n",
    "        # Calculate the information gain for the current attribute\n",
    "        result = gain_information(dataframe, attribut_cible, test_attribute)\n",
    "\n",
    "        # If the result is None, skip to the next attribute\n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        # Unpack the result from gain_information\n",
    "        _, current_gain, split_value, partitions = result\n",
    "\n",
    "        # Update the variables if the current gain is higher than the best gain\n",
    "        if current_gain > best_gain:\n",
    "            best_gain = current_gain\n",
    "            best_attribute = test_attribute\n",
    "            best_split_value = split_value\n",
    "            best_partitions = partitions\n",
    "\n",
    "    # Return the best attribute, gain, split value, and partitions\n",
    "    return best_attribute, best_gain, best_split_value, best_partitions\n",
    "\n",
    "\n",
    "find_best_gain(data, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(dataframe, attribut_cible, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data to split.\n",
    "    attribut_cible (str): The target attribute we want to predict. e.g (\"Class\")\n",
    "    test_size (float): The proportion of the data to include in the test split. Default is 0.2.\n",
    "    random_state (int): Controls the shuffling applied to the data before applying the split. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - X_train (pd.DataFrame): The training features.\n",
    "        - X_test (pd.DataFrame): The testing features.\n",
    "        - y_train (pd.Series): The training target attribute.\n",
    "        - y_test (pd.Series): The testing target attribute.\n",
    "    \"\"\"\n",
    "    # Separate features and target attribute\n",
    "    X = dataframe.drop(columns=[attribut_cible])\n",
    "    y = dataframe[attribut_cible]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test =  split_data(data, \"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(\n",
    "            self,\n",
    "            is_leaf,\n",
    "            attribute=None,\n",
    "            split_value=None,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            prediction=None):\n",
    "        \"\"\"\n",
    "        Initialize a decision tree node.\n",
    "\n",
    "        Parameters:\n",
    "        is_leaf (bool): Whether the node is a leaf node.\n",
    "        attribute (str, optional): The attribute to split on if the node is not a leaf.\n",
    "        split_value (float, optional): The split value for the attribute if the node is not a leaf.\n",
    "        left (DecisionNode, optional): The left child node.\n",
    "        right (DecisionNode, optional): The right child node.\n",
    "        value (object, optional): The target value if the node is a leaf.\n",
    "        \"\"\"\n",
    "        self.is_leaf = is_leaf\n",
    "        self.attribute = attribute\n",
    "        self.split_value = split_value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.prediction = prediction\n",
    "\n",
    "    def _is_leaf(self):\n",
    "        return self.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Represents a decision tree.\n",
    "\n",
    "    Attributes:\n",
    "    max_depth (int): The maximum depth of the tree.\n",
    "    tree (DecisionNode): The root node of the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=8):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        max_depth (int, optional): The maximum depth of the tree. Default is 8.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def _build_tree(self, data, target_attribute, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The input data as a pandas DataFrame.\n",
    "        target_attribute (str): The target attribute that we want to predict.\n",
    "        depth (int): The current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "        DecisionNode: The root node of the decision tree.\n",
    "        \"\"\"\n",
    "        # Check stopping conditions: maximum depth or pure leaf\n",
    "        if depth >= self.max_depth:\n",
    "            # Return a leaf node with the most frequent target value\n",
    "            prediction = data[target_attribute].mode()[0]\n",
    "            return DecisionNode(is_leaf=True, prediction=prediction)\n",
    "\n",
    "        # Check if the data is pure (all target values are the same)\n",
    "        if data[target_attribute].nunique() == 1:\n",
    "            # Return a leaf node with the unique target value\n",
    "            prediction = data[target_attribute].iloc[0]\n",
    "            return DecisionNode(is_leaf=True, prediction=prediction)\n",
    "\n",
    "        # Find the best attribute, gain, split value, and partitions using find_best_gain\n",
    "        best_attribute, best_gain, best_split_value, best_partitions = find_best_gain(\n",
    "            data, target_attribute)\n",
    "\n",
    "        # Check if no gain is found, return the most frequent target value as a leaf node\n",
    "        if best_attribute is None or best_gain <= 0:\n",
    "            prediction = data[target_attribute].mode()[0]\n",
    "            return DecisionNode(is_leaf=True, prediction=prediction)\n",
    "\n",
    "        # Create the decision node with the best attribute and split value\n",
    "        left_data, right_data = best_partitions\n",
    "        left_child = self._build_tree(left_data, target_attribute, depth + 1)\n",
    "        right_child = self._build_tree(right_data, target_attribute, depth + 1)\n",
    "\n",
    "        # Return the decision node with the children\n",
    "        return DecisionNode(\n",
    "            is_leaf=False,\n",
    "            attribute=best_attribute,\n",
    "            split_value=best_split_value,\n",
    "            left=left_child,\n",
    "            right=right_child\n",
    "        )\n",
    "\n",
    "    def fit(self, data, target_attribute):\n",
    "        \"\"\"\n",
    "        Build the decision tree based on the provided data and target attribute. e.g (\"Class\")\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The input data as a pandas DataFrame.\n",
    "        target_attribute (str): The target attribute that we want to predict.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(data, target_attribute)\n",
    "\n",
    "    def train(self, dataframe, target_attribute):\n",
    "        \"\"\"\n",
    "        Train the decision tree using the given data.\n",
    "\n",
    "        Parameters:\n",
    "        dataframe (pd.DataFrame): The DataFrame containing the training data.\n",
    "        target_attribute (str): The target attribute we want to predict.\n",
    "        \"\"\"\n",
    "        self.fit(dataframe, target_attribute)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Predict target attribute values for the given data using the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The data for which predictions are to be made.\n",
    "\n",
    "        Returns:\n",
    "        np.array: The predicted values of the target attribute.\n",
    "        \"\"\"\n",
    "        predictions = data.apply(self._predict_single, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def _predict_single(self, row):\n",
    "        \"\"\"\n",
    "        Predict the target attribute value for a single data point.\n",
    "\n",
    "        Parameters:\n",
    "        row (pd.Series): The data point as a pandas Series.\n",
    "\n",
    "        Returns:\n",
    "        object: The predicted value of the target attribute.\n",
    "        \"\"\"\n",
    "        node = self.tree\n",
    "\n",
    "        # Traverse the tree until a leaf node is reached\n",
    "        while not node.is_leaf:\n",
    "            attribute = node.attribute\n",
    "            split_value = node.split_value\n",
    "\n",
    "            if row[attribute] < split_value:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        # Return the value of the leaf node\n",
    "        return node.prediction\n",
    "\n",
    "    def print_tree(self, node=None, indent=\"\"):\n",
    "        \"\"\"\n",
    "        Print the decision tree in a human-readable way.\n",
    "\n",
    "        Parameters:\n",
    "        node (DecisionNode, optional): The current node to print. If not specified, starts with the root node.\n",
    "        indent (str): Indentation for nested levels in the tree.\n",
    "        \"\"\"\n",
    "        # If no node is specified, start with the root node\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "\n",
    "        # Check if the current node is a leaf node\n",
    "        if node._is_leaf():\n",
    "            print(f\"{indent}Leaf: Predict {node.prediction}\")\n",
    "        else:\n",
    "            # Print the split condition and value at the current node\n",
    "            print(f\"{indent}Node: {node.attribute} < {node.split_value}\")\n",
    "\n",
    "            # Recursively print the left and right children\n",
    "            print(f\"{indent}Left:\")\n",
    "            self.print_tree(node.left, indent + \"    \")\n",
    "\n",
    "            print(f\"{indent}Right:\")\n",
    "            self.print_tree(node.right, indent + \"    \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class qui permet d'évaluer notre model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    \"\"\"\n",
    "    Classe pour calculer les métriques d'évaluation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def accuracy_score(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calcule l'exactitude entre les valeurs réelles et prédites.\n",
    "        \n",
    "        Arguments:\n",
    "        y_true -- Les valeurs réelles (cibles).\n",
    "        y_pred -- Les valeurs prédites par le modèle.\n",
    "        \n",
    "        Retourne:\n",
    "        float -- L'exactitude.\n",
    "        \"\"\"\n",
    "        correct_predictions = sum(y_true == y_pred)\n",
    "        total_predictions = len(y_true)\n",
    "        return correct_predictions / total_predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_score(y_true, y_pred, positive_label):\n",
    "        \"\"\"\n",
    "        Calcule la précision pour une classe positive spécifiée.\n",
    "        \n",
    "        Arguments:\n",
    "        y_true -- Les valeurs réelles (cibles).\n",
    "        y_pred -- Les valeurs prédites par le modèle.\n",
    "        positive_label -- La classe positive pour laquelle calculer la précision.\n",
    "        \n",
    "        Retourne:\n",
    "        float -- La précision.\n",
    "        \"\"\"\n",
    "        true_positives = sum((y_true == positive_label) & (y_pred == positive_label))\n",
    "        predicted_positives = sum(y_pred == positive_label)\n",
    "        if predicted_positives == 0:\n",
    "            return 0.0\n",
    "        return true_positives / predicted_positives\n",
    "\n",
    "    @staticmethod\n",
    "    def recall_score(y_true, y_pred, positive_label):\n",
    "        \"\"\"\n",
    "        Calcule le rappel pour une classe positive spécifiée.\n",
    "        \n",
    "        Arguments:\n",
    "        y_true -- Les valeurs réelles (cibles).\n",
    "        y_pred -- Les valeurs prédites par le modèle.\n",
    "        positive_label -- La classe positive pour laquelle calculer le rappel.\n",
    "        \n",
    "        Retourne:\n",
    "        float -- Le rappel.\n",
    "        \"\"\"\n",
    "        true_positives = sum((y_true == positive_label) & (y_pred == positive_label))\n",
    "        actual_positives = sum(y_true == positive_label)\n",
    "        if actual_positives == 0:\n",
    "            return 0.0\n",
    "        return true_positives / actual_positives\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(y_true, y_pred, positive_label):\n",
    "        \"\"\"\n",
    "        Calcule le score F1 pour une classe positive spécifiée.\n",
    "        \n",
    "        Arguments:\n",
    "        y_true -- Les valeurs réelles (cibles).\n",
    "        y_pred -- Les valeurs prédites par le modèle.\n",
    "        positive_label -- La classe positive pour laquelle calculer le score F1.\n",
    "        \n",
    "        Retourne:\n",
    "        float -- Le score F1.\n",
    "        \"\"\"\n",
    "        precision = EvaluationMetrics.precision_score(y_true, y_pred, positive_label)\n",
    "        recall = EvaluationMetrics.recall_score(y_true, y_pred, positive_label)\n",
    "        \n",
    "        if precision == 0 and recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * (precision * recall) / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_decision_trees(X_train, y_train, X_test, y_test, depths):\n",
    "    \"\"\"\n",
    "    Train decision tree models with different maximum depths and evaluate their performance.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features (data).\n",
    "        y_train: Training target (labels).\n",
    "        X_test: Testing features (data).\n",
    "        y_test: Testing target (labels).\n",
    "        depths (list): List of maximum depths to test for decision trees.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples containing the accuracy scores and corresponding models.\n",
    "        The list is sorted in descending order based on accuracy scores.\n",
    "    \"\"\"\n",
    "    # Combine features and labels for training and testing sets\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    # List to store the models and their accuracy scores\n",
    "    models_and_scores = []\n",
    "\n",
    "    # Train and evaluate models with different maximum depths\n",
    "    for max_depth in depths:\n",
    "        # Create a decision tree classifier with the specified maximum depth\n",
    "        model = DecisionTree(max_depth=max_depth)\n",
    "        \n",
    "        # Train the model on the combined training set\n",
    "        model.train(train_data, y_train.name)  # Pass the name of the target attribute\n",
    "        \n",
    "        # Predict on the combined test set\n",
    "        y_pred = model.predict(test_data)\n",
    "        \n",
    "        # Calculate the accuracy of the model using the provided y_test\n",
    "        accuracy = EvaluationMetrics.accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Append the accuracy score and model to the list\n",
    "        models_and_scores.append((accuracy, model))\n",
    "        \n",
    "        # Print the results for the current model\n",
    "        print(f\"Max Depth: {max_depth}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Sort the list of models and scores in descending order based on accuracy\n",
    "    models_and_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    return models_and_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attribute  = \"Class\"\n",
    "X_train, X_test, y_train, y_test = split_data(data, target_attribute)\n",
    "models_and_scores = train_and_evaluate_decision_trees(X_train, y_train, X_test, y_test, [3, 4, 5, 6, 7, 8])\n",
    "print(models_and_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models_and_scores[0]\n",
    "second_best_model = models_and_scores[1]\n",
    "\n",
    "print(\"Best model\", best_model)\n",
    "print(\"Second Best model\", second_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of DecisionNode and DecisionTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the target attribute\n",
    "target_attribute = \"Class\"\n",
    "\n",
    "# Split the data into training and testing sets using the split_data function\n",
    "X_train, X_test, y_train, y_test = split_data(data, target_attribute)\n",
    "\n",
    "# Instantiate the DecisionTree class\n",
    "decision_tree = DecisionTree(max_depth=8)\n",
    "evaluation_metric = EvaluationMetrics()\n",
    "\n",
    "# Train the decision tree using the training data\n",
    "decision_tree.train(pd.concat([X_train, y_train], axis=1), target_attribute)\n",
    "\n",
    "# Predict the target attribute for the testing data\n",
    "predictions = decision_tree.predict(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "# Evaluate the model using accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "eval_accuracy = evaluation_metric.accuracy_score(y_test, predictions)\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "eval_precision = evaluation_metric.precision_score(y_test, predictions, positive_label=1)\n",
    "\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "eval_recall = evaluation_metric.recall_score(y_test, predictions, positive_label=1)\n",
    "\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "eval_f1 = evaluation_metric.f1_score(y_test, predictions, positive_label=1)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Eval Accuracy: {eval_accuracy:.2f}\")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Eval Precision: {eval_precision:.2f}\")\n",
    "\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Eval Recall: {eval_recall:.2f}\")\n",
    "\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Eval F1-score: {eval_f1:.2f}\")\n",
    "\n",
    "decision_tree.print_tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Réseaux de neurones artificiels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Division des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Attr_A     Attr_B       Attr_C      Attr_D      Attr_E       Attr_F  \\\n",
      "1131  11.845628  11.161878  1313.712979  111.581496   95.456558   972.046868   \n",
      "351    8.499002  10.638715   776.313616  101.128788   98.194311  1101.968011   \n",
      "944    7.378612  10.231545   864.338533   97.541152  116.114975  1177.874002   \n",
      "1958   9.887848   9.274918   980.163706  105.475302  107.331859  1017.637187   \n",
      "210   11.062933  11.315338   699.887951   90.718138   96.913945  1083.308853   \n",
      "...         ...        ...          ...         ...         ...          ...   \n",
      "1638  10.500580   8.488398  1030.144905  102.816959  135.236907   829.134392   \n",
      "1095  10.075134   9.747951   749.562376   95.946030   87.677656  1180.735377   \n",
      "1130  10.228703   9.811083   964.832208   94.333244  102.811652   973.889034   \n",
      "1294   9.614717  11.321535   943.832125   92.273154  102.943806  1094.288656   \n",
      "860   12.609813   9.373724  1055.615571   95.149818  106.666943   988.819215   \n",
      "\n",
      "         Attr_G     Attr_H     Attr_I      Attr_J       Attr_K     Attr_L  \\\n",
      "1131   9.787987   8.550444  12.128631  148.804575   961.663118   8.861849   \n",
      "351    8.877056  11.547967   7.396834  103.639262   924.662366  10.927528   \n",
      "944    8.554622  11.850087   4.897704   87.269130   990.209816   9.363341   \n",
      "1958   9.608401   8.496842  10.794240  112.506015  1064.363895   9.814022   \n",
      "210   10.703455   9.384819  11.108118   57.485166  1059.523836  10.113602   \n",
      "...         ...        ...        ...         ...          ...        ...   \n",
      "1638   9.879802  12.127433   7.710255   86.368428  1212.644183   9.741186   \n",
      "1095  10.126631   8.601556   8.967080   89.217644   831.615020  10.945471   \n",
      "1130  10.036836   8.606357  10.520051   93.922696  1159.078265  11.511145   \n",
      "1294   9.803798   8.812094  11.254228   68.815528  1266.888276  11.365058   \n",
      "860   10.254045  11.531074  12.029041  111.203052  1033.988531   8.659581   \n",
      "\n",
      "           Attr_M      Attr_N  \n",
      "1131  1041.095333   69.896959  \n",
      "351   1202.955479  121.597810  \n",
      "944    952.818948  125.189304  \n",
      "1958  1147.272274   97.163378  \n",
      "210    981.521917  123.540695  \n",
      "...           ...         ...  \n",
      "1638  1195.365675  101.517404  \n",
      "1095   897.196814  114.706503  \n",
      "1130   982.915600   84.145803  \n",
      "1294   924.465918   99.994353  \n",
      "860   1174.804125   86.584889  \n",
      "\n",
      "[1838 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def neural_split_data(data, target_attribute):\n",
    "    \n",
    "    # # Séparez les données et les labels\n",
    "    # X_train, y_train = train_data.drop(\n",
    "    #     columns=[target_attribute]), train_data[target_attribute]\n",
    "    # X_val, y_val = val_data.drop(\n",
    "    #     columns=[target_attribute]), val_data[target_attribute]\n",
    "    # X_test, y_test = test_data.drop(\n",
    "    #     columns=[target_attribute]), test_data[target_attribute]\n",
    "    \n",
    "    # # Divisez les données en jeu d'entraînement et de test (85% et 15%)\n",
    "    # train_data, test_data = train_test_split(\n",
    "    #     data, test_size=0.15, random_state=42)\n",
    "\n",
    "    # # Divisez le jeu d'entraînement en sous-ensemble d'entraînement et de validation (85% et 15%)\n",
    "    # train_data, val_data = train_test_split(\n",
    "    #     train_data, test_size=0.15/0.85, random_state=42)\n",
    "\n",
    "    # Séparez les données et les labels afin de pouvoir séparer en jeu d'entrainement\n",
    "    # de test et de validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[target_attribute]),\n",
    "                                                        data[target_attribute], test_size=0.15, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15/0.85,\n",
    "                                                    random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "print (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Création de la classe \"NeuralNetwork\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, architecture, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with a specific architecture and activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - architecture (list of int): List of integers representing the number of neurons in each layer.\n",
    "        - activation (str): The activation function to use ('relu' or 'softmax').\n",
    "        \"\"\"\n",
    "        self.architecture = architecture\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases for the network layers.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        layer_sizes = self.architecture\n",
    "\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            weight_matrix = np.random.randn(\n",
    "                layer_sizes[i], layer_sizes[i - 1]) * 0.01\n",
    "            bias_vector = np.zeros((layer_sizes[i], 1))\n",
    "\n",
    "            self.weights.append(weight_matrix)\n",
    "            self.biases.append(bias_vector)\n",
    "            # print(f\"Dimensions de la matrice de poids {i}: {weight_matrix.shape}\")\n",
    "            # print(bias_vector.shape)\n",
    "\n",
    "    def activation_function(self, x, layer_index=None):\n",
    "        \"\"\"\n",
    "        Apply the specified activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x (np.array): Input array.\n",
    "\n",
    "        Returns:\n",
    "        - np.array: Output array with activation applied.\n",
    "        \"\"\"\n",
    "        if layer_index == len(self.architecture) - 2:\n",
    "            # Use softmax for the last hidden layer for classification\n",
    "            exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid activation function. Use 'tanh' or 'relu'.\")\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the specified activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x (np.array): Input array.\n",
    "\n",
    "        Returns:\n",
    "        - np.array: Derivative of the activation function.\n",
    "        \"\"\"\n",
    "        if self.activation == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        elif self.activation == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid activation function. Use 'tanh' or 'relu'.\")\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.array): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - List of activations and z-values at each layer.\n",
    "        \"\"\"\n",
    "        activations = [X.T]\n",
    "        # print(activations)\n",
    "        # print(activations)\n",
    "        zs = []\n",
    "\n",
    "        for i in range(len(self.architecture) - 1):\n",
    "            # print(f\"{self.weights[i].shape} * {activations[-1].shape} + {self.biases[i].shape}\")\n",
    "            z = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            activation = self.activation_function(z, i)\n",
    "            activations.append(activation)\n",
    "\n",
    "            # print(\"Forme de z : \", z.shape)\n",
    "        return activations, zs\n",
    "\n",
    "    def backward_propagation(self, activations, zs, y):\n",
    "        \"\"\"\n",
    "        Perform backward propagation and calculate gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - activations (list of np.array): List of activations at each layer.\n",
    "        - zs (list of np.array): List of z-values at each layer.\n",
    "        - y (np.array): True labels.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple containing gradients of weights and biases.\n",
    "        \"\"\"\n",
    "        m = y.shape[1]\n",
    "        dw = []\n",
    "        db = []\n",
    "\n",
    "        # Compute the gradient of the last layer\n",
    "        # Difference between predicted and actual labels\n",
    "        dz = activations[-1] - y\n",
    "\n",
    "        # Backpropagate the gradients\n",
    "        for i in range(len(self.architecture) - 1, 0, -1):\n",
    "            dw_i = np.dot(dz, activations[i - 1].T) / m\n",
    "            db_i = np.sum(dz, axis=1, keepdims=True) / m\n",
    "\n",
    "            dw.append(dw_i)\n",
    "            db.append(db_i)\n",
    "\n",
    "            if i > 1:\n",
    "                dz = np.dot(self.weights[i - 1].T, dz) * \\\n",
    "                    self.activation_derivative(zs[i - 2])\n",
    "\n",
    "        # Reverse gradients to match the order of weights and biases\n",
    "        return dw[::-1], db[::-1]\n",
    "\n",
    "    def update_parameters(self, dw, db, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the network parameters (weights and biases) using the given gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - dw (list of np.array): Gradients of weights.\n",
    "        - db (list of np.array): Gradients of biases.\n",
    "        - learning_rate (float): Learning rate for the update.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * dw[i]\n",
    "            self.biases[i] -= learning_rate * db[i]\n",
    "\n",
    "    def _shuffle_data(self, X_train, y_train):\n",
    "        N, _ = X_train.shape\n",
    "        shuffled_idx = np.random.permutation(N)\n",
    "        return X_train.iloc[shuffled_idx], y_train.iloc[shuffled_idx]\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=100, patience=4, batch_size=4):\n",
    "        \"\"\"\n",
    "        Train the network using the given data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train (np.array): Training data.\n",
    "        - y_train (np.array): Training labels.\n",
    "        - X_val (np.array): Validation data.\n",
    "        - y_val (np.array): Validation labels.\n",
    "        - learning rate (float): Learning rate for training.\n",
    "        - epochs (int): Number of epochs to train for.\n",
    "        - patience (int): Patience for early stopping.\n",
    "        - batch_size (int): Size of mini-batch for training.\n",
    "        \"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        N, D = X_train.shape\n",
    "        # print(f\"X_train.shape = {X_train.shape}\")\n",
    "\n",
    "        # print(X_train.shape)\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            X_train_shuffled, y_train_shuffled = self._shuffle_data(\n",
    "                X_train, y_train)\n",
    "\n",
    "            # Mini-batch training\n",
    "            num_batches = N // batch_size\n",
    "            print(f\"num_batches {num_batches}\")\n",
    "            for batch_idx in range(num_batches):\n",
    "                # Récupérez le mini-lot\n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                \n",
    "                # Utilisez X_train_shuffled[start:end] pour obtenir le mini-lot\n",
    "                X_batch = X_train_shuffled[start:end]\n",
    "               \n",
    "                # Transposez X_batch pour avoir les dimensions (14, batch_size)\n",
    "                X_batch = np.array(X_batch)\n",
    "\n",
    "                # Utilisez y_train_shuffled[start:end] pour obtenir les étiquettes du mini-lot\n",
    "                y_batch = y_train_shuffled[start:end]\n",
    "                y_batch = np.array(y_batch)\n",
    "                y_batch = np.eye(4)[y_batch]\n",
    "                # y_batch = y_batch.reshape((4, 1))\n",
    "\n",
    "                # print(f\"X_bath = {X_batch.shape}\")\n",
    "                # print(f\"Y_batch = {y_batch.shape}\")\n",
    "\n",
    "            \n",
    "                # Effectuez la propagation avant\n",
    "                activations, zs = self.forward_propagation(X_batch)\n",
    "\n",
    "                # Effectuez la propagation arrière\n",
    "                dw, db = self.backward_propagation(activations, zs, y_batch)\n",
    "\n",
    "                # Mettez à jour les paramètres\n",
    "                self.update_parameters(dw, db, learning_rate)\n",
    "\n",
    "            # Calculate loss on validation data\n",
    "            val_loss = self.calculate_loss(X_val, y_val)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "            # Display progress\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}: Validation loss = {val_loss:.4f}\")\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate loss using categorical cross-entropy.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.array): Input data.\n",
    "        - y (np.array): True labels.\n",
    "\n",
    "        Returns:\n",
    "        - float: Calculated loss.\n",
    "        \"\"\"\n",
    "        # Forward propagation\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        activations_finales = activations[-1].T  # Transposer activations[-1]\n",
    "        y = np.eye(4)[y]\n",
    "\n",
    "        # Affiche les formes des matrices pour déboguer\n",
    "        # print(\"Forme de y :\", y.shape)\n",
    "        # print(\"Forme de activations[-1] :\", activations[-1].shape)\n",
    "        # return\n",
    "\n",
    "\n",
    "        # Calculate loss using categorical cross-entropy\n",
    "        m = y.shape[0]  # Number of entries.\n",
    "        epsilon = 1e-9  # Small constant to avoid division by zero\n",
    "        loss = -np.sum(y * np.log(activations_finales + epsilon)) / m\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels for the given input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.array): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - np.array: Predicted labels.\n",
    "        \"\"\"\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        # Use np.argmax to return the class index with the highest probability\n",
    "        return np.argmax(activations[-1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifiez le nom de la colonne cible\n",
    "target_attribute = \"Class\"\n",
    "\n",
    "# Divisez les données en ensembles d'entraînement, de validation et de test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = neural_split_data(\n",
    "    data, target_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, y_train, X_val, y_val, architectures, activations, learning_rate=0.01, epochs=100, patience=4, batch_size=4):\n",
    "    \"\"\"\n",
    "    Train multiple neural network models with specified architectures and activations.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.array): Training data.\n",
    "    - y_train (np.array): Training labels.\n",
    "    - X_val (np.array): Validation data.\n",
    "    - y_val (np.array): Validation labels.\n",
    "    - architectures (list of lists): List of architectures (list of integers) to train.\n",
    "    - activations (list of str): List of activation functions to use ('tanh' and 'relu').\n",
    "    - learning_rate (float): Learning rate for training.\n",
    "    - epochs (int): Number of epochs to train for.\n",
    "    - patience (int): Patience for early stopping.\n",
    "    - batch_size (int): Size of mini-batch for training.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing trained models for each architecture and activation function.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "\n",
    "    # Train models for each architecture and activation function\n",
    "    for activation in activations:\n",
    "        for architecture in architectures:\n",
    "            # Create neural network model\n",
    "            model = NeuralNetwork(architecture, activation)\n",
    "\n",
    "            # Train the model\n",
    "            model.train(X_train, y_train, X_val, y_val, learning_rate=learning_rate,\n",
    "                        epochs=epochs, patience=patience, batch_size=batch_size)\n",
    "\n",
    "            # Store the trained model\n",
    "            models[f\"{activation}_{architecture}\"] = model\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [(14, 10, 8, 6, 4), (14, 10, 8, 4, 4), (14, 6, 4, 4)]\n",
    "activations = [\"tanh\", \"relu\"]\n",
    "models = train_models(X_train, y_train, X_val, y_val, architectures, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate trained models on the test data.\n",
    "\n",
    "    Parameters:\n",
    "    - models (dict): Dictionary containing trained models.\n",
    "    - X_test (np.array): Test data.\n",
    "    - y_test (np.array): Test labels.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing predictions and evaluation metrics for each model.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        # Predict the test data\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate loss (or other evaluation metrics)\n",
    "        \n",
    "        loss = model.calculate_loss(X_test, y_test)\n",
    "\n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            \"predictions\": predictions,\n",
    "            \"loss\": loss,\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_models(models, X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifiez le nom de la colonne cible\n",
    "target_attribute = \"Class\"\n",
    "\n",
    "# Divisez les données en ensembles d'entraînement, de validation et de test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = neural_split_data(data, target_attribute)\n",
    "\n",
    "# print(np.arange(X_train.shape[0]))\n",
    "# indices = np.arange(X_train.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# print(indices)\n",
    "# shuffled = X_train.iloc[indices]\n",
    "# print(shuffled)\n",
    "# print(f\"X_train shape {X_train.shape}\")\n",
    "# print(\"X_train\", X_train.head())\n",
    "# print(f\"y_train shape {y_train.shape}\")\n",
    "# print(f\"y_train  {y_train.head()}\")\n",
    "\n",
    "# print(f\"Valeurs uniques dans y_train : {np.unique(y_train)}\")\n",
    "\n",
    "# Liste des architectures à tester\n",
    "architectures = [(14, 10, 8, 6, 4)]\n",
    "\n",
    "# Fonction d'activation à tester\n",
    "activation = 'tanh'  # Utilisez 'tanh' ou 'relu'\n",
    "\n",
    "neural_network = NeuralNetwork(architecture=architectures[0])\n",
    "neural_network.train(X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=50, patience=4, batch_size=4)\n",
    "\n",
    "\n",
    "# # Boucle pour tester chaque architecture\n",
    "# for architecture in architectures:\n",
    "#     print(f\"Testing architecture: {architecture} with activation: {activation}\")\n",
    "\n",
    "#     # Créez une instance de NeuralNetwork avec l'architecture et l'activation spécifiées\n",
    "#     neural_network = NeuralNetwork(architecture, activation)\n",
    "\n",
    "#     # Entraînez le réseau de neurones\n",
    "#     neural_network.train(X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=50, patience=4, batch_size=4)\n",
    "\n",
    "#     # Effectuez les prédictions sur le jeu de test\n",
    "#     predictions = neural_network.predict(X_test)\n",
    "\n",
    "# #     # Évaluez les prédictions (par exemple, calcul de la précision)\n",
    "#     accuracy = np.mean(predictions == y_test)\n",
    "    \n",
    "# #     # Affichez les résultats de la précision\n",
    "#     print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
