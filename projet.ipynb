{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Attr_A     Attr_B       Attr_C      Attr_D      Attr_E       Attr_F  \\\n",
      "0  12.478064  14.784992  1247.541877  100.962061   52.462177  1089.398211   \n",
      "1   8.264345   8.854181  1389.686814   99.526529   65.106526  1186.523399   \n",
      "2   9.875571  13.292442   779.077401  123.993772  104.699796  1201.722480   \n",
      "3   9.207661   9.346913   965.468523   89.176009  102.628284   743.913507   \n",
      "4   8.863842  12.542969  1096.386230  106.595385  131.813380   883.059615   \n",
      "\n",
      "      Attr_G     Attr_H     Attr_I      Attr_J       Attr_K     Attr_L  \\\n",
      "0  10.575834   8.375407  10.288159  110.746551   994.367610   9.069350   \n",
      "1   9.500485  10.088058   9.371983   78.210274   943.089589   9.988919   \n",
      "2   9.545266  14.266238   9.703551   86.252483  1082.989190  10.084217   \n",
      "3   9.777953  11.613946   8.912059   96.727873   812.800511   8.621781   \n",
      "4  10.092974  13.556029  11.649982   21.566576   971.083175  10.072271   \n",
      "\n",
      "        Attr_M      Attr_N  Class  \n",
      "0  1027.953917  109.672758      1  \n",
      "1  1120.317724   83.498764      3  \n",
      "2   970.953682   93.557046      2  \n",
      "3   947.207195  120.890054      3  \n",
      "4  1007.583900  149.511979      2  \n"
     ]
    }
   ],
   "source": [
    "# Exo 1 - Préparation des données \n",
    "\n",
    "# importation des données\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "# import du fichier\n",
    "data = pd.read_csv(\"synthetic.csv\")\n",
    "\n",
    "# Visualisation des données\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Nombre de colonnes (attributs) dans le DataFrame\n",
    "num_attributes = data.shape[1]\n",
    "\n",
    "# Afficher le nombre d'attributs\n",
    "print(f\"Le nombre d'attributs dans le fichier est : {num_attributes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type de données et valeurs manquantes\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoir le nombre d'attributs dans le modèle\n",
    "print(data.columns)\n",
    "# 14 attributs dans le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les classes uniques dans la colonne 'Class'\n",
    "classes_uniques = data['Class'].unique()\n",
    "\n",
    "# Nombre de classes différentes\n",
    "num_classes = len(classes_uniques)\n",
    "\n",
    "# Afficher le nombre de classes différentes\n",
    "print(f\"Le nombre de classes différentes dans les données est : {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combien d'instances compte chaque classe?\n",
    "nbr_instances = data['Class'].value_counts()\n",
    "print(nbr_instances)\n",
    "\n",
    "# Sortie \n",
    "# Class\n",
    "# 1    908\n",
    "# 0    674\n",
    "# 2    472\n",
    "# 3    244\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les données sont-elles linéairement séparables ?\n",
    "Non, si on observe le schéma 1 on voit que les données ne le sont pas.\n",
    "De plus si l'on choisit de les ranger par classe , on peut s'apercevoir que \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # import biblio matplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['Attr_A'], data['Attr_B'], c=data['Class'], alpha=0.5, cmap='viridis')\n",
    "plt.xlabel('Attribut 1')\n",
    "plt.ylabel('Attribut 2')\n",
    "plt.title('Scatter Plot des attributs par classe')\n",
    "plt.colorbar(label='Classe')\n",
    "plt.show()\n",
    "\n",
    "# On peut voir clairement que ce n'est pas divisible linéairement à l'état brut\n",
    "# je pense que use image est vraiment mieux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 et 6 (voir compte-rendu.md) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Mise en oeuvre des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir un attribut à analyser, par exemple 'Attr_A'\n",
    "attribute = 'Attr_A'\n",
    "\n",
    "\n",
    "# Calculer les quartiles pour l'attribut choisi\n",
    "quartiles = data[attribute].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# Sort the attribute values and print them\n",
    "sorted_attribute = data[attribute].sort_values()\n",
    "print(sorted_attribute)\n",
    "print(quartiles)\n",
    "# Afficher les quartiles\n",
    "print(f\"Quartile 1 (Q1) de l'attribut '{attribute}': {quartiles[0.25]}\")\n",
    "print(f\"Médiane (Q2) de l'attribut '{attribute}': {quartiles[0.5]}\")\n",
    "print(f\"Quartile 3 (Q3) de l'attribut '{attribute}': {quartiles[0.75]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Mise en oeuvre des modèles.\n",
    "\n",
    "# Arbre de décision \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbre de décision\n",
    "\n",
    "# Calcul de l'entropie\n",
    "\n",
    "\"\"\"\n",
    "L'entropie est une mesure de l'incertitude associée à une variable aléatoire.\n",
    "\"\"\"\n",
    "\n",
    "def entropie(dataframe , attribut_cible):  \n",
    "    # Calcul de la probabilité de chaque classe\n",
    "    compte_classe = dataframe[attribut_cible].value_counts()\n",
    "    #print(compte_classe)\n",
    "    proba = compte_classe / compte_classe.sum()\n",
    "    #print(proba) \n",
    "    # Calcul de l'entropie\n",
    "    entropie = - (proba * np.log2(proba+ np.finfo(float).eps)).sum() # éviter log2(0)\n",
    "    return entropie\n",
    "\n",
    "# Test de la fonction\n",
    "print(entropie(data, 'Attr_A'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "1.8608867211835993\n",
    "1.860886721183598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.099235683587334, 10.41650269976578, 11.729265932954785]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour calculer tous les quartiles d'un attribut donné\n",
    "def calculate_quartiles(data, attribute):\n",
    "    quartiles =data[attribute].quantile([0.25, 0.5, 0.75])\n",
    "    return quartiles\n",
    "\n",
    "# Test de la fonction sur le DataFrame chargé\n",
    "\n",
    "print(calculate_quartiles(data, 'Attr_A'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"Attr_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()\n",
    "sorted = data.sort_values(by=\"Attr_A\")\n",
    "print(len(sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/marcolyantoine/Desktop/ia/projet.ipynb Cellule 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m attribut_test, max_gain, best_split_value, best_partitions\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m# Testing the function with an example attribute\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Let's use 'Attr_A' as the attribute to test and 'Class' as the target\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m test_gain_info \u001b[39m=\u001b[39m gain_information(data, \u001b[39m'\u001b[39m\u001b[39mClass\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAttr_H\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m test_gain_info\n",
      "\u001b[1;32m/Users/marcolyantoine/Desktop/ia/projet.ipynb Cellule 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m sorted_data \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39mattribut_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Unique values of the attribute to test, considering quartiles to reduce complexity\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m quartiles \u001b[39m=\u001b[39m calculate_quartiles(sorted_data, attribut_test)\u001b[39m.\u001b[39mto_list()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Adding the min and max values to cover the entire range of the attribute\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# quartiles = [sorted_data[attribut_test].min()] + quartiles + \\\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m#     [sorted_data[attribut_test].max()]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Voir si je n'enlève pas min et max valeur\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Iterating through the sorted unique values to find the best split\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m split_value \u001b[39min\u001b[39;00m quartiles:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/projet.ipynb#Y113sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# Partitioning the data based on the split value\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "def gain_information(dataframe, attribut_cible, attribut_test):\n",
    "    \"\"\"\n",
    "    Calculate the information gain from splitting the data based on a test attribute.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data to partition.\n",
    "    attribut_cible (str): The target attribute we want to predict.\n",
    "    attribut_test (str): The attribute whose gain we want to calculate.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - attribut_test (str): The test attribute.\n",
    "        - max_gain (float): The maximum information gain obtained.\n",
    "        - best_split_value (float): The split value that provides the best gain.\n",
    "        - best_partitions (tuple): A tuple containing two DataFrames representing the lower and upper partitions\n",
    "          resulting from the best split.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial entropy of the target attribute\n",
    "    entropie_initiale = entropie(dataframe, attribut_cible)\n",
    "\n",
    "    # The gain, split_value and partitions initialized\n",
    "    max_gain = 0\n",
    "    best_split_value = None\n",
    "    best_partitions = None\n",
    "\n",
    "    # Check for no unique values in the attribute being tested\n",
    "    if len(dataframe[attribut_test].unique()) <= 1:\n",
    "        return None\n",
    "\n",
    "    # Sorting data by the attribute to test\n",
    "    sorted_data = dataframe.sort_values(by=attribut_test)\n",
    "\n",
    "    # Unique values of the attribute to test, considering quartiles to reduce complexity\n",
    "    quartiles = calculate_quartiles(sorted_data, attribut_test).to_list()\n",
    "\n",
    "    # Adding the min and max values to cover the entire range of the attribute\n",
    "    # quartiles = [sorted_data[attribut_test].min()] + quartiles + \\\n",
    "    #     [sorted_data[attribut_test].max()]\n",
    "    # Voir si je n'enlève pas min et max valeur\n",
    "\n",
    "    # Iterating through the sorted unique values to find the best split\n",
    "    for split_value in quartiles:\n",
    "        # Partitioning the data based on the split value\n",
    "        lower_partition = sorted_data[sorted_data[attribut_test] < split_value]\n",
    "        upper_partition = sorted_data[sorted_data[attribut_test]\n",
    "                                      >= split_value]\n",
    "\n",
    "        # Calculating the weighted entropy for the partitions\n",
    "        # Row counts.\n",
    "        total_instances = len(sorted_data)\n",
    "        lower_weight = len(lower_partition) / total_instances\n",
    "        upper_weight = len(upper_partition) / total_instances\n",
    "\n",
    "        # Computing the weighted_entropy\n",
    "        weighted_entropy = (lower_weight * entropie(lower_partition, attribut_cible)) + \\\n",
    "                           (upper_weight * entropie(upper_partition, attribut_cible))\n",
    "\n",
    "        # Information gain for the current split\n",
    "        current_gain = entropie_initiale - weighted_entropy\n",
    "\n",
    "        # If the current gain is greater than the max_gain, update max_gain and best_split_value\n",
    "        if current_gain > max_gain:\n",
    "            max_gain = current_gain\n",
    "            best_split_value = split_value\n",
    "            best_partitions = (lower_partition, upper_partition)\n",
    "\n",
    "    # Returning the attribute, gain, split_value, and partitions as a tuple\n",
    "    return attribut_test, max_gain, best_split_value, best_partitions\n",
    "\n",
    "\n",
    "# Testing the function with an example attribute\n",
    "# Let's use 'Attr_A' as the attribute to test and 'Class' as the target\n",
    "test_gain_info = gain_information(data, 'Class', 'Attr_H')\n",
    "test_gain_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ff/drt5s5zx5rl8tlv4_v2v9tyh0000gn/T/ipykernel_3711/3017609013.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  lower_partition = pd.concat([lower_partition, moving_data])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Attr_H',\n",
       " 0.13650018458513413,\n",
       " 10.193379573020025,\n",
       " (         Attr_A     Attr_B       Attr_C      Attr_D      Attr_E       Attr_F  \\\n",
       "  1121   6.377450  11.228294   886.426642  104.982647  109.170561   684.266146   \n",
       "  245    4.781749  14.022814  1185.219922  120.079881  134.873534  1019.790528   \n",
       "  715    8.782567   9.683032  1268.478960   88.060288   85.606156   897.779906   \n",
       "  12     8.390581  12.352995  1012.305252   96.187491  111.643709   972.983129   \n",
       "  773    9.526369  14.308478  1219.807051  112.871810   58.286968   861.457110   \n",
       "  ...         ...        ...          ...         ...         ...          ...   \n",
       "  1099   6.920209  13.730635  1131.588009   99.176107  107.291776   965.230881   \n",
       "  631   12.531046   7.211853  1172.063524  100.153066  136.905073   760.395873   \n",
       "  1713  12.611922   9.192021  1577.536407   89.312730  128.772804   612.147859   \n",
       "  1435  11.582174   9.974185  1165.170936  103.687963   77.781597  1239.079486   \n",
       "  1653  11.835193  10.565540  1096.681067  108.676601   57.149183  1144.504341   \n",
       "  \n",
       "           Attr_G     Attr_H     Attr_I      Attr_J       Attr_K     Attr_L  \\\n",
       "  1121   9.309472   2.046314  10.525000  100.614996  1002.967182  11.324278   \n",
       "  245    9.167396   3.110049  10.954618  105.817684   970.988464   9.474028   \n",
       "  715   11.162487   3.964169   9.651860  136.261349   923.391158  10.010943   \n",
       "  12    10.238824   4.068899   9.074872  100.572615   981.583448  10.964436   \n",
       "  773    8.923077   4.132270   9.778012  123.647015  1072.603879  10.437498   \n",
       "  ...         ...        ...        ...         ...          ...        ...   \n",
       "  1099   7.767718  10.184461   9.282119   88.799799  1013.077868   9.190445   \n",
       "  631    9.389508  10.184846   7.814130   91.608035  1108.891051   8.626676   \n",
       "  1713   9.651594  10.186476  10.806673  108.183728   986.108286  11.072709   \n",
       "  1435  10.180744  10.187096  10.952762  126.016900  1059.891272   8.537072   \n",
       "  1653   9.441718  10.190360  10.508304   70.205760   996.350072  11.215775   \n",
       "  \n",
       "             Attr_M      Attr_N Class  \n",
       "  1121   901.254953  103.154283     1  \n",
       "  245   1062.248333   46.466341     1  \n",
       "  715   1116.434750  108.601468     1  \n",
       "  12    1189.788584   86.410903     1  \n",
       "  773    943.486863  119.427064     1  \n",
       "  ...           ...         ...   ...  \n",
       "  1099  1012.367644  102.986575     2  \n",
       "  631    880.181925  104.636034     0  \n",
       "  1713  1002.070868   71.524133     0  \n",
       "  1435  1044.285349  108.358747     1  \n",
       "  1653   987.662109   82.813968     0  \n",
       "  \n",
       "  [1149 rows x 15 columns],\n",
       "           Attr_A     Attr_B       Attr_C      Attr_D      Attr_E       Attr_F  \\\n",
       "  826   11.958492  10.351152  1112.263068  100.559207  102.772203   993.570428   \n",
       "  1580  14.238403   7.933350   993.851803  101.769592  108.997127  1008.249442   \n",
       "  2150  13.339669  12.876209  1258.164832  108.942409   65.326622  1130.521075   \n",
       "  1708  10.361981  11.338420  1140.903595   92.013860   82.866593  1234.341980   \n",
       "  1847   9.134969   5.150550   741.096047  100.222235  104.891824  1486.199645   \n",
       "  ...         ...        ...          ...         ...         ...          ...   \n",
       "  554    5.136832   8.117328  1060.807300  102.043813  135.385532   818.882626   \n",
       "  2126  13.343966  11.237435  1105.990304  103.433553   33.638937   654.799729   \n",
       "  878    9.303017   9.401485   655.085896   96.815964  102.141102   523.668197   \n",
       "  562    6.080178   8.699372  1074.922101  104.810368  132.589371   928.827361   \n",
       "  2219   5.919370   6.769393  1325.342010   96.432435   87.084522  1051.377426   \n",
       "  \n",
       "           Attr_G     Attr_H     Attr_I      Attr_J       Attr_K     Attr_L  \\\n",
       "  826    9.864729  10.196400  12.569331  126.295543   823.729301   9.622654   \n",
       "  1580  11.270812  10.196558  10.609397  106.057188  1235.033912  11.116564   \n",
       "  2150  12.045280  10.198870   9.848603  126.091776  1074.240207  10.610564   \n",
       "  1708   9.321634  10.199325   6.743490  111.812429   982.503239   9.499647   \n",
       "  1847  10.744649  10.201003   4.092349   91.672123   953.395198  10.348496   \n",
       "  ...         ...        ...        ...         ...          ...        ...   \n",
       "  554   10.129547  15.747682  11.960625   42.769802  1006.638185   9.852355   \n",
       "  2126  10.976983  15.872521   9.377233   91.623174   976.641027  10.842069   \n",
       "  878    9.983926  16.154612   8.413253   98.633515  1072.727938   9.927936   \n",
       "  562    7.734452  16.257121  11.710000   37.359495  1116.911804  11.113738   \n",
       "  2219   8.898090  16.564235  10.829712   80.183539  1069.301372   9.105430   \n",
       "  \n",
       "             Attr_M      Attr_N  Class  \n",
       "  826    983.419998   74.661539      1  \n",
       "  1580   812.114309   94.993678      0  \n",
       "  2150   950.064317   85.191038      1  \n",
       "  1708  1024.756928  101.748493      3  \n",
       "  1847  1016.966462  136.543299      2  \n",
       "  ...           ...         ...    ...  \n",
       "  554   1088.558323  151.303481      3  \n",
       "  2126  1219.883704   43.363590      0  \n",
       "  878    897.420642   60.508888      0  \n",
       "  562   1012.621847  144.226031      3  \n",
       "  2219  1006.627507  144.595592      3  \n",
       "  \n",
       "  [1149 rows x 15 columns]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_variance(dataframe, attribut_test, threshold=0.01):\n",
    "    return dataframe[attribut_test].var() >= threshold\n",
    "\n",
    "\n",
    "def gain_information_optimized(dataframe, attribut_cible, attribut_test):\n",
    "    # Si l'attribut a très peu de valeurs uniques, éviter de l'utiliser pour le split\n",
    "    if dataframe[attribut_test].nunique() <= 1:\n",
    "        return attribut_test, 0, None, None\n",
    "\n",
    "    # vérifier variance     \n",
    "    if not check_variance(dataframe, attribut_test):\n",
    "        return attribut_test, 0, None, None\n",
    "\n",
    "\n",
    "    # Tri des données par l'attribut à tester \n",
    "    sorted_data = dataframe.sort_values(by=attribut_test)\n",
    "    total_count = len(sorted_data)\n",
    "    entropie_initiale = entropie(dataframe, attribut_cible)\n",
    "\n",
    "    max_gain = 0\n",
    "    best_split_value = None\n",
    "    best_partitions = None\n",
    "\n",
    "    # Calcul des quartiles\n",
    "    quartiles = calculate_quartiles(sorted_data, attribut_test)\n",
    "    thresholds = quartiles \n",
    "\n",
    "    # Initialisation des partitions\n",
    "    lower_partition = pd.DataFrame(columns=sorted_data.columns)\n",
    "    upper_partition = sorted_data.copy()\n",
    "\n",
    "    previous_threshold = thresholds[0]\n",
    "\n",
    "    for threshold in thresholds[1:]:\n",
    "        # Déplacer les données de upper à lower basées sur le seuil actuel\n",
    "        mask = (upper_partition[attribut_test] < threshold)\n",
    "        moving_data = upper_partition[mask]\n",
    "        lower_partition = pd.concat([lower_partition, moving_data])\n",
    "        upper_partition = upper_partition[~mask]\n",
    "\n",
    "        # Vérifier si les partitions sont vides\n",
    "        if lower_partition.empty or upper_partition.empty:\n",
    "            continue\n",
    "\n",
    "        # Calcul de l'entropie pondérée pour les partitions actuelles\n",
    "        lower_weight = len(lower_partition) / total_count\n",
    "        upper_weight = len(upper_partition) / total_count\n",
    "\n",
    "        weighted_entropy = (lower_weight * entropie(lower_partition, attribut_cible) +\n",
    "                            upper_weight * entropie(upper_partition, attribut_cible))\n",
    "        current_gain = entropie_initiale - weighted_entropy\n",
    "\n",
    "        # Mise à jour du meilleur gain si nécessaire\n",
    "        if current_gain > max_gain:\n",
    "            max_gain = current_gain\n",
    "            best_split_value = threshold\n",
    "            best_partitions = (lower_partition.copy(), upper_partition.copy())\n",
    "\n",
    "    return attribut_test, max_gain, best_split_value, best_partitions\n",
    "\n",
    "\n",
    "\n",
    "# Testing the function with an example attribute\n",
    "# Let's use 'Attr_A' as the attribute to test and 'Class' as the target\n",
    "test_gain_info = gain_information_optimized(data, 'Class', 'Attr_H')\n",
    "test_gain_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_gain(dataframe, attribut_cible):\n",
    "    \"\"\"\n",
    "    Calculate the best information gain and corresponding split in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The input data as a pandas DataFrame.\n",
    "    attribut_cible (str): The target attribute that we want to predict (e.g. 'Class').\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the following elements:\n",
    "        - best_attribute (str): The attribute that yields the best information gain.\n",
    "        - best_gain (float): The highest information gain observed.\n",
    "        - best_split_value (float): The split value that produces the best gain.\n",
    "        - best_partitions (tuple): A tuple containing the two partitions resulting from the best split.\n",
    "    \"\"\"\n",
    "    # Initialize variables to track the best gain and the associated attribute\n",
    "    best_gain = 0\n",
    "    best_attribute = None\n",
    "    best_split_value = None\n",
    "    best_partitions = None\n",
    "\n",
    "    # Iterate over all the attributes in the DataFrame, except the target attribute\n",
    "    for test_attribute in dataframe.columns:\n",
    "        if test_attribute == attribut_cible:\n",
    "            continue  # Skip the target attribute\n",
    "\n",
    "        # Calculate the information gain for the current attribute\n",
    "        result = gain_information(dataframe, attribut_cible, test_attribute)\n",
    "\n",
    "        # If the result is None, skip to the next attribute\n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        # Unpack the result from gain_information\n",
    "        _, current_gain, split_value, partitions = result\n",
    "\n",
    "        # Update the variables if the current gain is higher than the best gain\n",
    "        if current_gain > best_gain:\n",
    "            best_gain = current_gain\n",
    "            best_attribute = test_attribute\n",
    "            best_split_value = split_value\n",
    "            best_partitions = partitions\n",
    "\n",
    "    # Return the best attribute, gain, split value, and partitions\n",
    "    return best_attribute, best_gain, best_split_value, best_partitions\n",
    "\n",
    "\n",
    "find_best_gain(data, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(dataframe, attribut_cible, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data to split.\n",
    "    attribut_cible (str): The target attribute we want to predict. e.g (\"Class\")\n",
    "    test_size (float): The proportion of the data to include in the test split. Default is 0.2.\n",
    "    random_state (int): Controls the shuffling applied to the data before applying the split. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - X_train (pd.DataFrame): The training features.\n",
    "        - X_test (pd.DataFrame): The testing features.\n",
    "        - y_train (pd.Series): The training target attribute.\n",
    "        - y_test (pd.Series): The testing target attribute.\n",
    "    \"\"\"\n",
    "    # Separate features and target attribute\n",
    "    X = dataframe.drop(columns=[attribut_cible])\n",
    "    y = dataframe[attribut_cible]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test =  split_data(data, \"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    def __init__(\n",
    "            self,\n",
    "            is_leaf,\n",
    "            attribute=None,\n",
    "            split_value=None,\n",
    "            left=None,\n",
    "            right=None,\n",
    "            prediction=None):\n",
    "        \"\"\"\n",
    "        Initialize a decision tree node.\n",
    "\n",
    "        Parameters:\n",
    "        is_leaf (bool): Whether the node is a leaf node.\n",
    "        attribute (str, optional): The attribute to split on if the node is not a leaf.\n",
    "        split_value (float, optional): The split value for the attribute if the node is not a leaf.\n",
    "        left (DecisionNode, optional): The left child node.\n",
    "        right (DecisionNode, optional): The right child node.\n",
    "        value (object, optional): The target value if the node is a leaf.\n",
    "        \"\"\"\n",
    "        self.is_leaf = is_leaf\n",
    "        self.attribute = attribute\n",
    "        self.split_value = split_value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.prediction = prediction\n",
    "\n",
    "    def _is_leaf(self):\n",
    "        return self.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Represents a decision tree.\n",
    "\n",
    "    Attributes:\n",
    "    max_depth (int): The maximum depth of the tree.\n",
    "    tree (DecisionNode): The root node of the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=8):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        max_depth (int, optional): The maximum depth of the tree. Default is 8.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def _build_tree(self, data, target_attribute, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The input data as a pandas DataFrame.\n",
    "        target_attribute (str): The target attribute that we want to predict.\n",
    "        depth (int): The current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "        DecisionNode: The root node of the decision tree.\n",
    "        \"\"\"\n",
    "        # Check stopping conditions: maximum depth or pure leaf\n",
    "        if depth >= self.max_depth:\n",
    "            # Return a leaf node with the most frequent target value\n",
    "            prediction = data[target_attribute].mode()[0]\n",
    "            return DecisionNode(is_leaf=True, prediction=prediction)\n",
    "\n",
    "        # Check if the data is pure (all target values are the same)\n",
    "        if data[target_attribute].nunique() == 1:\n",
    "            # Return a leaf node with the unique target value\n",
    "            prediction = data[target_attribute].iloc[0]\n",
    "            return DecisionNode(is_leaf=True, prediction=prediction)\n",
    "\n",
    "        # Find the best attribute, gain, split value, and partitions using find_best_gain\n",
    "        best_attribute, best_gain, best_split_value, best_partitions = find_best_gain(\n",
    "            data, target_attribute)\n",
    "\n",
    "        # Check if no gain is found, return the most frequent target value as a leaf node\n",
    "        if best_attribute is None or best_gain <= 0:\n",
    "            prediction = data[target_attribute].mode()[0]\n",
    "            return DecisionNode(is_leaf=True, prediction=prediction)\n",
    "\n",
    "        # Create the decision node with the best attribute and split value\n",
    "        left_data, right_data = best_partitions\n",
    "        left_child = self._build_tree(left_data, target_attribute, depth + 1)\n",
    "        right_child = self._build_tree(right_data, target_attribute, depth + 1)\n",
    "\n",
    "        # Return the decision node with the children\n",
    "        return DecisionNode(\n",
    "            is_leaf=False,\n",
    "            attribute=best_attribute,\n",
    "            split_value=best_split_value,\n",
    "            left=left_child,\n",
    "            right=right_child\n",
    "        )\n",
    "\n",
    "    def fit(self, data, target_attribute):\n",
    "        \"\"\"\n",
    "        Build the decision tree based on the provided data and target attribute. e.g (\"Class\")\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The input data as a pandas DataFrame.\n",
    "        target_attribute (str): The target attribute that we want to predict.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(data, target_attribute)\n",
    "\n",
    "    def train(self, dataframe, target_attribute):\n",
    "        \"\"\"\n",
    "        Train the decision tree using the given data.\n",
    "\n",
    "        Parameters:\n",
    "        dataframe (pd.DataFrame): The DataFrame containing the training data.\n",
    "        target_attribute (str): The target attribute we want to predict.\n",
    "        \"\"\"\n",
    "        self.fit(dataframe, target_attribute)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Predict target attribute values for the given data using the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        data (pd.DataFrame): The data for which predictions are to be made.\n",
    "\n",
    "        Returns:\n",
    "        np.array: The predicted values of the target attribute.\n",
    "        \"\"\"\n",
    "        predictions = data.apply(self._predict_single, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def _predict_single(self, row):\n",
    "        \"\"\"\n",
    "        Predict the target attribute value for a single data point.\n",
    "\n",
    "        Parameters:\n",
    "        row (pd.Series): The data point as a pandas Series.\n",
    "\n",
    "        Returns:\n",
    "        object: The predicted value of the target attribute.\n",
    "        \"\"\"\n",
    "        node = self.tree\n",
    "\n",
    "        # Traverse the tree until a leaf node is reached\n",
    "        while not node.is_leaf:\n",
    "            attribute = node.attribute\n",
    "            split_value = node.split_value\n",
    "\n",
    "            if row[attribute] < split_value:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        # Return the value of the leaf node\n",
    "        return node.prediction\n",
    "\n",
    "    def print_tree(self, node=None, indent=\"\"):\n",
    "        \"\"\"\n",
    "        Print the decision tree in a human-readable way.\n",
    "\n",
    "        Parameters:\n",
    "        node (DecisionNode, optional): The current node to print. If not specified, starts with the root node.\n",
    "        indent (str): Indentation for nested levels in the tree.\n",
    "        \"\"\"\n",
    "        # If no node is specified, start with the root node\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "\n",
    "        # Check if the current node is a leaf node\n",
    "        if node._is_leaf():\n",
    "            print(f\"{indent}Leaf: Predict {node.prediction}\")\n",
    "        else:\n",
    "            # Print the split condition and value at the current node\n",
    "            print(f\"{indent}Node: {node.attribute} < {node.split_value}\")\n",
    "\n",
    "            # Recursively print the left and right children\n",
    "            print(f\"{indent}Left:\")\n",
    "            self.print_tree(node.left, indent + \"    \")\n",
    "\n",
    "            print(f\"{indent}Right:\")\n",
    "            self.print_tree(node.right, indent + \"    \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class qui permet d'évaluer notre model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    \"\"\"\n",
    "    Classe pour calculer les métriques d'évaluation.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def accuracy_score(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calcule l'exactitude entre les valeurs réelles et prédites.\n",
    "        \n",
    "        Arguments:\n",
    "        y_true -- Les valeurs réelles (cibles).\n",
    "        y_pred -- Les valeurs prédites par le modèle.\n",
    "        \n",
    "        Retourne:\n",
    "        float -- L'exactitude.\n",
    "        \"\"\"\n",
    "        correct_predictions = sum(y_true == y_pred)\n",
    "        total_predictions = len(y_true)\n",
    "        return correct_predictions / total_predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_score(y_true, y_pred, positive_label):\n",
    "        \"\"\"\n",
    "        Calcule la précision pour une classe positive spécifiée.\n",
    "        \n",
    "        Arguments:\n",
    "        y_true -- Les valeurs réelles (cibles).\n",
    "        y_pred -- Les valeurs prédites par le modèle.\n",
    "        positive_label -- La classe positive pour laquelle calculer la précision.\n",
    "        \n",
    "        Retourne:\n",
    "        float -- La précision.\n",
    "        \"\"\"\n",
    "        true_positives = sum((y_true == positive_label) & (y_pred == positive_label))\n",
    "        predicted_positives = sum(y_pred == positive_label)\n",
    "        if predicted_positives == 0:\n",
    "            return 0.0\n",
    "        return true_positives / predicted_positives\n",
    "\n",
    "    @staticmethod\n",
    "    def recall_score(y_true, y_pred, positive_label):\n",
    "        \"\"\"\n",
    "        Calcule le rappel pour une classe positive spécifiée.\n",
    "        \n",
    "        Arguments:\n",
    "        y_true -- Les valeurs réelles (cibles).\n",
    "        y_pred -- Les valeurs prédites par le modèle.\n",
    "        positive_label -- La classe positive pour laquelle calculer le rappel.\n",
    "        \n",
    "        Retourne:\n",
    "        float -- Le rappel.\n",
    "        \"\"\"\n",
    "        true_positives = sum((y_true == positive_label) & (y_pred == positive_label))\n",
    "        actual_positives = sum(y_true == positive_label)\n",
    "        if actual_positives == 0:\n",
    "            return 0.0\n",
    "        return true_positives / actual_positives\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(y_true, y_pred, positive_label):\n",
    "        \"\"\"\n",
    "        Calcule le score F1 pour une classe positive spécifiée.\n",
    "        \n",
    "        Arguments:\n",
    "        y_true -- Les valeurs réelles (cibles).\n",
    "        y_pred -- Les valeurs prédites par le modèle.\n",
    "        positive_label -- La classe positive pour laquelle calculer le score F1.\n",
    "        \n",
    "        Retourne:\n",
    "        float -- Le score F1.\n",
    "        \"\"\"\n",
    "        precision = EvaluationMetrics.precision_score(y_true, y_pred, positive_label)\n",
    "        recall = EvaluationMetrics.recall_score(y_true, y_pred, positive_label)\n",
    "        \n",
    "        if precision == 0 and recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return 2 * (precision * recall) / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_decision_trees(X_train, y_train, X_test, y_test, depths):\n",
    "    \"\"\"\n",
    "    Train decision tree models with different maximum depths and evaluate their performance.\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features (data).\n",
    "        y_train: Training target (labels).\n",
    "        X_test: Testing features (data).\n",
    "        y_test: Testing target (labels).\n",
    "        depths (list): List of maximum depths to test for decision trees.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples containing the accuracy scores and corresponding models.\n",
    "        The list is sorted in descending order based on accuracy scores.\n",
    "    \"\"\"\n",
    "    # Combine features and labels for training and testing sets\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    # List to store the models and their accuracy scores\n",
    "    models_and_scores = []\n",
    "\n",
    "    # Train and evaluate models with different maximum depths\n",
    "    for max_depth in depths:\n",
    "        # Create a decision tree classifier with the specified maximum depth\n",
    "        model = DecisionTree(max_depth=max_depth)\n",
    "        \n",
    "        # Train the model on the combined training set\n",
    "        model.train(train_data, y_train.name)  # Pass the name of the target attribute\n",
    "        \n",
    "        # Predict on the combined test set\n",
    "        y_pred = model.predict(test_data)\n",
    "        \n",
    "        # Calculate the accuracy of the model using the provided y_test\n",
    "        accuracy = EvaluationMetrics.accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Append the accuracy score and model to the list\n",
    "        models_and_scores.append((accuracy, model))\n",
    "        \n",
    "        # Print the results for the current model\n",
    "        print(f\"Max Depth: {max_depth}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Sort the list of models and scores in descending order based on accuracy\n",
    "    models_and_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    return models_and_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attribute  = \"Class\"\n",
    "X_train, X_test, y_train, y_test = split_data(data, target_attribute)\n",
    "models_and_scores = train_and_evaluate_decision_trees(X_train, y_train, X_test, y_test, [3, 4, 5, 6, 7, 8])\n",
    "print(models_and_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models_and_scores[0]\n",
    "second_best_model = models_and_scores[1]\n",
    "\n",
    "print(\"Best model\", best_model)\n",
    "print(\"Second Best model\", second_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of DecisionNode and DecisionTree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define the target attribute\n",
    "target_attribute = \"Class\"\n",
    "\n",
    "# Split the data into training and testing sets using the split_data function\n",
    "X_train, X_test, y_train, y_test = split_data(data, target_attribute)\n",
    "\n",
    "# Instantiate the DecisionTree class\n",
    "decision_tree = DecisionTree(max_depth=8)\n",
    "evaluation_metric = EvaluationMetrics()\n",
    "\n",
    "# Train the decision tree using the training data\n",
    "decision_tree.train(pd.concat([X_train, y_train], axis=1), target_attribute)\n",
    "\n",
    "# Predict the target attribute for the testing data\n",
    "predictions = decision_tree.predict(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "# Evaluate the model using accuracy, precision, recall, and F1-score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "eval_accuracy = evaluation_metric.accuracy_score(y_test, predictions)\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "eval_precision = evaluation_metric.precision_score(y_test, predictions, positive_label=1)\n",
    "\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "eval_recall = evaluation_metric.recall_score(y_test, predictions, positive_label=1)\n",
    "\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "eval_f1 = evaluation_metric.f1_score(y_test, predictions, positive_label=1)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Eval Accuracy: {eval_accuracy:.2f}\")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Eval Precision: {eval_precision:.2f}\")\n",
    "\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Eval Recall: {eval_recall:.2f}\")\n",
    "\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"Eval F1-score: {eval_f1:.2f}\")\n",
    "\n",
    "decision_tree.print_tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Réseaux de neurones artificiels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Division des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def neural_split_data(data, target_attribute):\n",
    "    \n",
    "    # # Séparez les données et les labels\n",
    "    # X_train, y_train = train_data.drop(\n",
    "    #     columns=[target_attribute]), train_data[target_attribute]\n",
    "    # X_val, y_val = val_data.drop(\n",
    "    #     columns=[target_attribute]), val_data[target_attribute]\n",
    "    # X_test, y_test = test_data.drop(\n",
    "    #     columns=[target_attribute]), test_data[target_attribute]\n",
    "    \n",
    "    # # Divisez les données en jeu d'entraînement et de test (85% et 15%)\n",
    "    # train_data, test_data = train_test_split(\n",
    "    #     data, test_size=0.15, random_state=42)\n",
    "\n",
    "    # # Divisez le jeu d'entraînement en sous-ensemble d'entraînement et de validation (85% et 15%)\n",
    "    # train_data, val_data = train_test_split(\n",
    "    #     train_data, test_size=0.15/0.85, random_state=42)\n",
    "\n",
    "    # Séparez les données et les labels afin de pouvoir séparer en jeu d'entrainement\n",
    "    # de test et de validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[target_attribute]),\n",
    "                                                        data[target_attribute], test_size=0.15, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15/0.85,\n",
    "                                                    random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "#print (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Création de la classe \"NeuralNetwork\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, architecture, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with a specific architecture and activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - architecture (list of int): List of integers representing the number of neurons in each layer.\n",
    "        - activation (str): The activation function to use ('relu' or 'tanh').\n",
    "        \"\"\"\n",
    "        self.architecture = architecture\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases for the network layers.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        layer_sizes = self.architecture\n",
    "\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            weight_matrix = np.random.randn(\n",
    "                layer_sizes[i], layer_sizes[i - 1]) * 0.01\n",
    "            bias_vector = np.zeros((layer_sizes[i], 1))\n",
    "\n",
    "            self.weights.append(weight_matrix)\n",
    "            self.biases.append(bias_vector)\n",
    "            # print(f\"Dimensions de la matrice de poids {i}: {weight_matrix.shape}\")\n",
    "            # print(bias_vector.shape)\n",
    "\n",
    "    def activation_function(self, x, layer_index=None):\n",
    "        \"\"\"\n",
    "        Apply the specified activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x (np.array): Input array.\n",
    "\n",
    "        Returns:\n",
    "        - np.array: Output array with activation applied.\n",
    "        \"\"\"\n",
    "        if layer_index == len(self.architecture) - 2:\n",
    "            # Use softmax for the last hidden layer for classification\n",
    "            exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid activation function. Use 'tanh' or 'relu'.\")\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the specified activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x (np.array): Input array.\n",
    "\n",
    "        Returns:\n",
    "        - np.array: Derivative of the activation function.\n",
    "        \"\"\"\n",
    "        if self.activation == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        elif self.activation == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid activation function. Use 'tanh' or 'relu'.\")\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Perform forward propagation.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.array): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - List of activations and z-values at each layer.\n",
    "        \"\"\"\n",
    "        activations = [X.T]\n",
    "        # print(activations)\n",
    "        # print(activations)\n",
    "        zs = []\n",
    "\n",
    "        for i in range(len(self.architecture) - 1):\n",
    "            # print(f\"{self.weights[i].shape} * {activations[-1].shape} + {self.biases[i].shape}\")\n",
    "            z = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
    "            zs.append(z)\n",
    "            activation = self.activation_function(z, i)\n",
    "            activations.append(activation)\n",
    "\n",
    "            # print(\"Forme de z : \", z.shape)\n",
    "        return activations, zs\n",
    "\n",
    "    def backward_propagation(self, activations, zs, y):\n",
    "        \"\"\"\n",
    "        Perform backward propagation and calculate gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - activations (list of np.array): List of activations at each layer.\n",
    "        - zs (list of np.array): List of z-values at each layer.\n",
    "        - y (np.array): True labels.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple containing gradients of weights and biases.\n",
    "        \"\"\"\n",
    "        m = y.shape[1]\n",
    "        dw = []\n",
    "        db = []\n",
    "\n",
    "        # Compute the gradient of the last layer\n",
    "        # Difference between predicted and actual labels\n",
    "        dz = activations[-1] - y\n",
    "\n",
    "        # Backpropagate the gradients\n",
    "        for i in range(len(self.architecture) - 1, 0, -1):\n",
    "            dw_i = np.dot(dz, activations[i - 1].T) / m\n",
    "            db_i = np.sum(dz, axis=1, keepdims=True) / m\n",
    "\n",
    "            dw.append(dw_i)\n",
    "            db.append(db_i)\n",
    "\n",
    "            if i > 1:\n",
    "                dz = np.dot(self.weights[i - 1].T, dz) * \\\n",
    "                    self.activation_derivative(zs[i - 2])\n",
    "\n",
    "        # Reverse gradients to match the order of weights and biases\n",
    "        return dw[::-1], db[::-1]\n",
    "\n",
    "    def update_parameters(self, dw, db, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the network parameters (weights and biases) using the given gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - dw (list of np.array): Gradients of weights.\n",
    "        - db (list of np.array): Gradients of biases.\n",
    "        - learning_rate (float): Learning rate for the update.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * dw[i]\n",
    "            self.biases[i] -= learning_rate * db[i]\n",
    "\n",
    "    def _shuffle_data(self, X_train, y_train):\n",
    "        N, _ = X_train.shape\n",
    "        shuffled_idx = np.random.permutation(N)\n",
    "        return X_train.iloc[shuffled_idx], y_train.iloc[shuffled_idx]\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=100, patience=4, batch_size=4):\n",
    "        \"\"\"\n",
    "        Train the network using the given data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train (np.array): Training data.\n",
    "        - y_train (np.array): Training labels.\n",
    "        - X_val (np.array): Validation data.\n",
    "        - y_val (np.array): Validation labels.\n",
    "        - learning rate (float): Learning rate for training.\n",
    "        - epochs (int): Number of epochs to train for.\n",
    "        - patience (int): Patience for early stopping.\n",
    "        - batch_size (int): Size of mini-batch for training.\n",
    "        \"\"\"\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        N, D = X_train.shape\n",
    "        # print(f\"X_train.shape = {X_train.shape}\")\n",
    "\n",
    "        # print(X_train.shape)\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data\n",
    "            X_train_shuffled, y_train_shuffled = self._shuffle_data(\n",
    "                X_train, y_train)\n",
    "\n",
    "            # Mini-batch training\n",
    "            num_batches = N // batch_size\n",
    "            print(f\"num_batches {num_batches}\")\n",
    "            for batch_idx in range(num_batches):\n",
    "                # Récupérez le mini-lot\n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                \n",
    "                # Utilisez X_train_shuffled[start:end] pour obtenir le mini-lot\n",
    "                X_batch = X_train_shuffled[start:end]\n",
    "               \n",
    "                # Transposez X_batch pour avoir les dimensions (14, batch_size)\n",
    "                X_batch = np.array(X_batch)\n",
    "\n",
    "                # Utilisez y_train_shuffled[start:end] pour obtenir les étiquettes du mini-lot\n",
    "                y_batch = y_train_shuffled[start:end]\n",
    "                y_batch = np.array(y_batch)\n",
    "                y_batch = np.eye(4)[y_batch]\n",
    "                # y_batch = y_batch.reshape((4, 1))\n",
    "\n",
    "                # print(f\"X_bath = {X_batch.shape}\")\n",
    "                # print(f\"Y_batch = {y_batch.shape}\")\n",
    "\n",
    "            \n",
    "                # Effectuez la propagation avant\n",
    "                activations, zs = self.forward_propagation(X_batch)\n",
    "\n",
    "                # Effectuez la propagation arrière\n",
    "                dw, db = self.backward_propagation(activations, zs, y_batch)\n",
    "\n",
    "                # Mettez à jour les paramètres\n",
    "                self.update_parameters(dw, db, learning_rate)\n",
    "\n",
    "            # Calculate loss on validation data\n",
    "            val_loss = self.calculate_loss(X_val, y_val)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "            # Display progress\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{epochs}: Validation loss = {val_loss:.4f}\")\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate loss using categorical cross-entropy.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.array): Input data.\n",
    "        - y (np.array): True labels.\n",
    "\n",
    "        Returns:\n",
    "        - float: Calculated loss.\n",
    "        \"\"\"\n",
    "        # Forward propagation\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        activations_finales = activations[-1].T  # Transposer activations[-1]\n",
    "        y = np.eye(4)[y]\n",
    "\n",
    "        # Affiche les formes des matrices pour déboguer\n",
    "        # print(\"Forme de y :\", y.shape)\n",
    "        # print(\"Forme de activations[-1] :\", activations[-1].shape)\n",
    "        # return\n",
    "\n",
    "\n",
    "        # Calculate loss using categorical cross-entropy\n",
    "        m = y.shape[0]  # Number of entries.\n",
    "        epsilon = 1e-9  # Small constant to avoid division by zero\n",
    "        loss = -np.sum(y * np.log(activations_finales + epsilon)) / m\n",
    "        return loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels for the given input data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (np.array): Input data.\n",
    "\n",
    "        Returns:\n",
    "        - np.array: Predicted labels.\n",
    "        \"\"\"\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        # Use np.argmax to return the class index with the highest probability\n",
    "        return np.argmax(activations[-1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifiez le nom de la colonne cible\n",
    "target_attribute = \"Class\"\n",
    "\n",
    "# Divisez les données en ensembles d'entraînement, de validation et de test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = neural_split_data(\n",
    "    data, target_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, y_train, X_val, y_val, architectures, activations, learning_rate=0.01, epochs=100, patience=4, batch_size=4):\n",
    "    \"\"\"\n",
    "    Train multiple neural network models with specified architectures and activations.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.array): Training data.\n",
    "    - y_train (np.array): Training labels.\n",
    "    - X_val (np.array): Validation data.\n",
    "    - y_val (np.array): Validation labels.\n",
    "    - architectures (list of lists): List of architectures (list of integers) to train.\n",
    "    - activations (list of str): List of activation functions to use ('tanh' and 'relu').\n",
    "    - learning_rate (float): Learning rate for training.\n",
    "    - epochs (int): Number of epochs to train for.\n",
    "    - patience (int): Patience for early stopping.\n",
    "    - batch_size (int): Size of mini-batch for training.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing trained models for each architecture and activation function.\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "\n",
    "    # Train models for each architecture and activation function\n",
    "    for activation in activations:\n",
    "        for architecture in architectures:\n",
    "            # Create neural network model\n",
    "            model = NeuralNetwork(architecture, activation)\n",
    "\n",
    "            # Train the model\n",
    "            model.train(X_train, y_train, X_val, y_val, learning_rate=learning_rate,\n",
    "                        epochs=epochs, patience=patience, batch_size=batch_size)\n",
    "\n",
    "            # Store the trained model\n",
    "            models[f\"{activation}_{architecture}\"] = model\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [(14, 10, 8, 6, 4), (14, 10, 8, 4, 4), (14, 6, 4, 4)]\n",
    "activations = [\"tanh\", \"relu\"]\n",
    "models = train_models(X_train, y_train, X_val, y_val, architectures, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate trained models on the test data.\n",
    "\n",
    "    Parameters:\n",
    "    - models (dict): Dictionary containing trained models.\n",
    "    - X_test (np.array): Test data.\n",
    "    - y_test (np.array): Test labels.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing predictions and evaluation metrics for each model.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        # Predict the test data\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Calculate loss (or other evaluation metrics)\n",
    "        \n",
    "        loss = model.calculate_loss(X_test, y_test)\n",
    "\n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            \"predictions\": predictions,\n",
    "            \"loss\": loss,\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_models(models, X_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécifiez le nom de la colonne cible\n",
    "target_attribute = \"Class\"\n",
    "\n",
    "# Divisez les données en ensembles d'entraînement, de validation et de test\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = neural_split_data(data, target_attribute)\n",
    "\n",
    "# print(np.arange(X_train.shape[0]))\n",
    "# indices = np.arange(X_train.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# print(indices)\n",
    "# shuffled = X_train.iloc[indices]\n",
    "# print(shuffled)\n",
    "# print(f\"X_train shape {X_train.shape}\")\n",
    "# print(\"X_train\", X_train.head())\n",
    "# print(f\"y_train shape {y_train.shape}\")\n",
    "# print(f\"y_train  {y_train.head()}\")\n",
    "\n",
    "# print(f\"Valeurs uniques dans y_train : {np.unique(y_train)}\")\n",
    "\n",
    "# Liste des architectures à tester\n",
    "architectures = [(14, 10, 8, 6, 4)]\n",
    "\n",
    "# Fonction d'activation à tester\n",
    "activation = 'tanh'  # Utilisez 'tanh' ou 'relu'\n",
    "\n",
    "neural_network = NeuralNetwork(architecture=architectures[0])\n",
    "neural_network.train(X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=50, patience=4, batch_size=4)\n",
    "\n",
    "\n",
    "# # Boucle pour tester chaque architecture\n",
    "# for architecture in architectures:\n",
    "#     print(f\"Testing architecture: {architecture} with activation: {activation}\")\n",
    "\n",
    "#     # Créez une instance de NeuralNetwork avec l'architecture et l'activation spécifiées\n",
    "#     neural_network = NeuralNetwork(architecture, activation)\n",
    "\n",
    "#     # Entraînez le réseau de neurones\n",
    "#     neural_network.train(X_train, y_train, X_val, y_val, learning_rate=0.01, epochs=50, patience=4, batch_size=4)\n",
    "\n",
    "#     # Effectuez les prédictions sur le jeu de test\n",
    "#     predictions = neural_network.predict(X_test)\n",
    "\n",
    "# #     # Évaluez les prédictions (par exemple, calcul de la précision)\n",
    "#     accuracy = np.mean(predictions == y_test)\n",
    "    \n",
    "# #     # Affichez les résultats de la précision\n",
    "#     print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Séparer les features et les labels\n",
    "X = data.drop('Label', axis=1)\n",
    "y = data['Label']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le réseau de neurones\n",
    "nn = NeuralNetwork(architecture=[(14, 10, 8, 6, 4)], activation='relu')  # Architecture avec 3 couches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le réseau\n",
    "nn.train(X_train, y_train, X_test, y_test, learning_rate=0.01, epochs=100, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédire les labels sur l'ensemble de test\n",
    "predictions = nn.predict(X_test)  # Assurez-vous que les dimensions correspondent aux attentes de votre classe\n",
    "# Calculer les métriques de performance\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
