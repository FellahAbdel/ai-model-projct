{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présentation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Exo 1 - Préparation des données \n",
    "\n",
    "# importation des données\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "# import du fichier\n",
    "data = pd.read_csv(\"synthetic.csv\")\n",
    "\n",
    "# Visualisation des données\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Nombre de colonnes (attributs) dans le DataFrame\n",
    "num_attributes = data.shape[1]\n",
    "\n",
    "# Afficher le nombre d'attributs\n",
    "print(f\"Le nombre d'attributs dans le fichier est : {num_attributes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type de données et valeurs manquantes\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoir le nombre d'attributs dans le modèle\n",
    "print(data.columns)\n",
    "# 14 attributs dans le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les classes uniques dans la colonne 'Class'\n",
    "classes_uniques = data['Class'].unique()\n",
    "\n",
    "# Nombre de classes différentes\n",
    "num_classes = len(classes_uniques)\n",
    "\n",
    "# Afficher le nombre de classes différentes\n",
    "print(f\"Le nombre de classes différentes dans les données est : {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combien d'instances compte chaque classe?\n",
    "nbr_instances = data['Class'].value_counts()\n",
    "print(nbr_instances)\n",
    "\n",
    "# Sortie \n",
    "# Class\n",
    "# 1    908\n",
    "# 0    674\n",
    "# 2    472\n",
    "# 3    244\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les données sont-elles linéairement séparables ?\n",
    "Non, si on observe le schéma 1 on voit que les données ne le sont pas.\n",
    "De plus si l'on choisit de les ranger par classe , on peut s'apercevoir que \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # import biblio matplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['Attr_A'], data['Attr_B'], c=data['Class'], alpha=0.5, cmap='viridis')\n",
    "plt.xlabel('Attribut 1')\n",
    "plt.ylabel('Attribut 2')\n",
    "plt.title('Scatter Plot des attributs par classe')\n",
    "plt.colorbar(label='Classe')\n",
    "plt.show()\n",
    "\n",
    "# On peut voir clairement que ce n'est pas divisible linéairement à l'état brut\n",
    "# je pense que use image est vraiment mieux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 et 6 (voir compte-rendu.md) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Mise en oeuvre des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir un attribut à analyser, par exemple 'Attr_A'\n",
    "attribute = 'Attr_A'\n",
    "\n",
    "\n",
    "# Calculer les quartiles pour l'attribut choisi\n",
    "quartiles = data[attribute].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# Sort the attribute values and print them\n",
    "sorted_attribute = data[attribute].sort_values()\n",
    "print(sorted_attribute)\n",
    "print(quartiles)\n",
    "# Afficher les quartiles\n",
    "print(f\"Quartile 1 (Q1) de l'attribut '{attribute}': {quartiles[0.25]}\")\n",
    "print(f\"Médiane (Q2) de l'attribut '{attribute}': {quartiles[0.5]}\")\n",
    "print(f\"Quartile 3 (Q3) de l'attribut '{attribute}': {quartiles[0.75]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Mise en oeuvre des modèles.\n",
    "\n",
    "# Arbre de décision \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbre de décision\n",
    "\n",
    "# Calcul de l'entropie\n",
    "\n",
    "\"\"\"\n",
    "L'entropie est une mesure de l'incertitude associée à une variable aléatoire.\n",
    "\"\"\"\n",
    "\n",
    "def entropie(dataframe , attribut_cible):  \n",
    "    # Calcul de la probabilité de chaque classe\n",
    "    compte_classe = dataframe[attribut_cible].value_counts()\n",
    "    #print(compte_classe)\n",
    "    proba = compte_classe / compte_classe.sum()\n",
    "    #print(proba) \n",
    "    # Calcul de l'entropie\n",
    "    entropie = - (proba * np.log2(proba+ np.finfo(float).eps)).sum() # éviter log2(0)\n",
    "    return entropie\n",
    "\n",
    "# Test de la fonction\n",
    "print(entropie(data, 'Attr_A'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "1.8608867211835993\n",
    "1.860886721183598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour calculer tous les quartiles d'un attribut donné\n",
    "def calculate_quartiles(data, attribute):\n",
    "    return data[attribute].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "# Test de la fonction sur le DataFrame chargé\n",
    "\n",
    "print(calculate_quartiles(data, 'Attr_A'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by=\"Attr_C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.head()\n",
    "sorted = data.sort_values(by=\"Attr_A\")\n",
    "print(len(sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_information(dataframe, attribut_cible, attribut_test):\n",
    "    \"\"\"\n",
    "    Calculate the information gain from splitting the data based on a test attribute.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data to partition.\n",
    "    attribut_cible (str): The target attribute we want to predict.\n",
    "    attribut_test (str): The attribute whose gain we want to calculate.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - attribut_test (str): The test attribute.\n",
    "        - max_gain (float): The maximum information gain obtained.\n",
    "        - best_split_value (float): The split value that provides the best gain.\n",
    "        - best_partitions (tuple): A tuple containing two DataFrames representing the lower and upper partitions\n",
    "          resulting from the best split.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initial entropy of the target attribute\n",
    "    entropie_initiale = entropie(dataframe, attribut_cible)\n",
    "\n",
    "    # The gain, split_value and partitions initialized\n",
    "    max_gain = 0\n",
    "    best_split_value = None\n",
    "    best_partitions = None\n",
    "\n",
    "    # Check for no unique values in the attribute being tested\n",
    "    if len(dataframe[attribut_test].unique()) <= 1:\n",
    "        return None\n",
    "\n",
    "    # Sorting data by the attribute to test\n",
    "    sorted_data = dataframe.sort_values(by=attribut_test)\n",
    "\n",
    "    # Unique values of the attribute to test, considering quartiles to reduce complexity\n",
    "    quartiles = calculate_quartiles(sorted_data, attribut_test).to_list()\n",
    "\n",
    "    # Adding the min and max values to cover the entire range of the attribute\n",
    "    quartiles = [sorted_data[attribut_test].min()] + quartiles + \\\n",
    "        [sorted_data[attribut_test].max()]\n",
    "    # Voir si je n'enlève pas min et max valeur\n",
    "\n",
    "    # Iterating through the sorted unique values to find the best split\n",
    "    for split_value in quartiles:\n",
    "        # Partitioning the data based on the split value\n",
    "        lower_partition = sorted_data[sorted_data[attribut_test] < split_value]\n",
    "        upper_partition = sorted_data[sorted_data[attribut_test]\n",
    "                                      >= split_value]\n",
    "\n",
    "        # Calculating the weighted entropy for the partitions\n",
    "        # Row counts.\n",
    "        total_instances = len(sorted_data)\n",
    "        lower_weight = len(lower_partition) / total_instances\n",
    "        upper_weight = len(upper_partition) / total_instances\n",
    "\n",
    "        # Computing the weighted_entropy\n",
    "        weighted_entropy = (lower_weight * entropie(lower_partition, attribut_cible)) + \\\n",
    "                           (upper_weight * entropie(upper_partition, attribut_cible))\n",
    "\n",
    "        # Information gain for the current split\n",
    "        current_gain = entropie_initiale - weighted_entropy\n",
    "\n",
    "        # If the current gain is greater than the max_gain, update max_gain and best_split_value\n",
    "        if current_gain > max_gain:\n",
    "            max_gain = current_gain\n",
    "            best_split_value = split_value\n",
    "            best_partitions = (lower_partition, upper_partition)\n",
    "\n",
    "    # Returning the attribute, gain, split_value, and partitions as a tuple\n",
    "    return attribut_test, max_gain, best_split_value, best_partitions\n",
    "\n",
    "\n",
    "# Testing the function with an example attribute\n",
    "# Let's use 'Attr_A' as the attribute to test and 'Class' as the target\n",
    "test_gain_info = gain_information(data, 'Class', 'Attr_H')\n",
    "test_gain_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_gain(dataframe, attribut_cible):\n",
    "    \"\"\"\n",
    "    Calculate the best information gain and corresponding split in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The input data as a pandas DataFrame.\n",
    "    target_attribute (str): The target attribute that we want to predict (e.g. 'Class').\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the following elements:\n",
    "        - best_attribute (str): The attribute that yields the best information gain.\n",
    "        - best_gain (float): The highest information gain observed.\n",
    "        - best_split_value (float): The split value that produces the best gain.\n",
    "        - best_partitions (tuple): A tuple containing the two partitions resulting from the best split.\n",
    "    \"\"\"\n",
    "    # Initialize variables to track the best gain and the associated attribute\n",
    "    best_gain = 0\n",
    "    best_attribute = None\n",
    "    best_split_value = None\n",
    "    best_partitions = None\n",
    "\n",
    "    # Iterate over all the attributes in the DataFrame, except the target attribute\n",
    "    for test_attribute in dataframe.columns:\n",
    "        if test_attribute == attribut_cible:\n",
    "            continue  # Skip the target attribute\n",
    "\n",
    "        # Calculate the information gain for the current attribute\n",
    "        result = gain_information(dataframe, attribut_cible, test_attribute)\n",
    "\n",
    "        # If the result is None, skip to the next attribute\n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        # Unpack the result from gain_information\n",
    "        _, current_gain, split_value, partitions = result\n",
    "\n",
    "        # Update the variables if the current gain is higher than the best gain\n",
    "        if current_gain > best_gain:\n",
    "            best_gain = current_gain\n",
    "            best_attribute = test_attribute\n",
    "            best_split_value = split_value\n",
    "            best_partitions = partitions\n",
    "\n",
    "    # Return the best attribute, gain, split value, and partitions\n",
    "    return best_attribute, best_gain, best_split_value, best_partitions\n",
    "\n",
    "\n",
    "find_best_gain(data, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(dataframe, attribut_cible, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data to split.\n",
    "    attribut_cible (str): The target attribute we want to predict.\n",
    "    test_size (float): The proportion of the data to include in the test split. Default is 0.2.\n",
    "    random_state (int): Controls the shuffling applied to the data before applying the split. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - X_train (pd.DataFrame): The training features.\n",
    "        - X_test (pd.DataFrame): The testing features.\n",
    "        - y_train (pd.Series): The training target attribute.\n",
    "        - y_test (pd.Series): The testing target attribute.\n",
    "    \"\"\"\n",
    "    # Separate features and target attribute\n",
    "    X = dataframe.drop(columns=[attribut_cible])\n",
    "    y = dataframe[attribut_cible]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics (accuracy, precision, recall, F1-score) for the model.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (pd.Series): The true target values.\n",
    "    y_pred (pd.Series): The predicted target values.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the metrics:\n",
    "        - accuracy (float): The accuracy of the model.\n",
    "        - precision (float): The precision of the model.\n",
    "        - recall (float): The recall of the model.\n",
    "        - f1_score (float): The F1-score of the model.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
