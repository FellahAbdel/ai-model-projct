{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {},
=======
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
>>>>>>> 550e255dbe9f603d6297cfe010121c3e76411872
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Attr_A     Attr_B       Attr_C      Attr_D      Attr_E       Attr_F  \\\n",
      "0  12.478064  14.784992  1247.541877  100.962061   52.462177  1089.398211   \n",
      "1   8.264345   8.854181  1389.686814   99.526529   65.106526  1186.523399   \n",
      "2   9.875571  13.292442   779.077401  123.993772  104.699796  1201.722480   \n",
      "3   9.207661   9.346913   965.468523   89.176009  102.628284   743.913507   \n",
      "4   8.863842  12.542969  1096.386230  106.595385  131.813380   883.059615   \n",
      "\n",
      "      Attr_G     Attr_H     Attr_I      Attr_J       Attr_K     Attr_L  \\\n",
      "0  10.575834   8.375407  10.288159  110.746551   994.367610   9.069350   \n",
      "1   9.500485  10.088058   9.371983   78.210274   943.089589   9.988919   \n",
      "2   9.545266  14.266238   9.703551   86.252483  1082.989190  10.084217   \n",
      "3   9.777953  11.613946   8.912059   96.727873   812.800511   8.621781   \n",
      "4  10.092974  13.556029  11.649982   21.566576   971.083175  10.072271   \n",
      "\n",
      "        Attr_M      Attr_N  Class  \n",
      "0  1027.953917  109.672758      1  \n",
      "1  1120.317724   83.498764      3  \n",
      "2   970.953682   93.557046      2  \n",
      "3   947.207195  120.890054      3  \n",
      "4  1007.583900  149.511979      2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10990/1950219857.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Exo 1 - Préparation des données \n",
    "\n",
    "# importation des données\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "# import du fichier\n",
    "data = pd.read_csv(\"synthetic.csv\")\n",
    "\n",
    "# Visualisation des données\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre d'attributs dans le fichier est : 15\n"
     ]
    }
   ],
   "source": [
    "# 1 - Nombre de colonnes (attributs) dans le DataFrame\n",
    "num_attributes = data.shape[1]\n",
    "\n",
    "# Afficher le nombre d'attributs\n",
    "print(f\"Le nombre d'attributs dans le fichier est : {num_attributes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2298 entries, 0 to 2297\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Attr_A  2298 non-null   float64\n",
      " 1   Attr_B  2298 non-null   float64\n",
      " 2   Attr_C  2298 non-null   float64\n",
      " 3   Attr_D  2298 non-null   float64\n",
      " 4   Attr_E  2298 non-null   float64\n",
      " 5   Attr_F  2298 non-null   float64\n",
      " 6   Attr_G  2298 non-null   float64\n",
      " 7   Attr_H  2298 non-null   float64\n",
      " 8   Attr_I  2298 non-null   float64\n",
      " 9   Attr_J  2298 non-null   float64\n",
      " 10  Attr_K  2298 non-null   float64\n",
      " 11  Attr_L  2298 non-null   float64\n",
      " 12  Attr_M  2298 non-null   float64\n",
      " 13  Attr_N  2298 non-null   float64\n",
      " 14  Class   2298 non-null   int64  \n",
      "dtypes: float64(14), int64(1)\n",
      "memory usage: 269.4 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Type de données et valeurs manquantes\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Attr_A', 'Attr_B', 'Attr_C', 'Attr_D', 'Attr_E', 'Attr_F', 'Attr_G',\n",
      "       'Attr_H', 'Attr_I', 'Attr_J', 'Attr_K', 'Attr_L', 'Attr_M', 'Attr_N',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Avoir le nombre d'attributs dans le modèle\n",
    "print(data.columns)\n",
    "# 14 attributs dans le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de classes différentes dans les données est : 4\n"
     ]
    }
   ],
   "source": [
    "# Obtenir les classes uniques dans la colonne 'Class'\n",
    "classes_uniques = data['Class'].unique()\n",
    "\n",
    "# Nombre de classes différentes\n",
    "num_classes = len(classes_uniques)\n",
    "\n",
    "# Afficher le nombre de classes différentes\n",
    "print(f\"Le nombre de classes différentes dans les données est : {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# combien d'instances compte chaque classe?\n",
    "nbr_instances = data['Class'].value_counts()\n",
    "print(nbr_instances)\n",
=======
    "# combien d'instanes ompte haque classe?\n",
    "distribution_classes = data['Class'].value_counts()\n",
    "print(distribution_classes)\n",
>>>>>>> 550e255dbe9f603d6297cfe010121c3e76411872
    "\n",
    "# Sortie \n",
    "# Class\n",
    "# 1    908\n",
    "# 0    674\n",
    "# 2    472\n",
    "# 3    244\n",
    "# Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les données sont-elles linéairement séparables ?\n",
    "Non, si on observe le schéma 1 on voit que les données ne le sont pas.\n",
    "De plus si l'on choisit de les ranger par classe , on peut s'apercevoir que \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # import biblio matplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['Attr_A'], data['Attr_B'], c=data['Class'], alpha=0.5, cmap='viridis')\n",
    "plt.xlabel('Attribut 1')\n",
    "plt.ylabel('Attribut 2')\n",
    "plt.title('Scatter Plot des attributs par classe')\n",
    "plt.colorbar(label='Classe')\n",
    "plt.show()\n",
    "\n",
    "# On peut voir clairement que ce n'est pas divisible linéairement à l'état brut\n",
    "# je pense que use image est vraiment mieux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - \n",
    "- Pour l'arbre de décision\n",
    "    Puisque les variables sont déjà numériques et que la colonne Class est utilisée comme étiquette (et non comme une fonctionnalité), aucun encodage One-hot n'est nécessaire pour les fonctionnalités. Si Class était utilisée comme une caractéristique d'entrée plutôt que comme une étiquette, et si elle comprenait de nombreuses catégories différentes, l'encodage One-hot pourrait être envisagé pour éviter de donner un ordre artificiel entre les catégories.\n",
    "    Pour un modèle basé sur un arbre de décision, la normalisation des données n'est généralement pas nécessaire. Les arbres de décision ne sont pas sensibles à la magnitude des valeurs des attributs de la même manière que le sont les modèles basés sur des calculs de distance ou des modèles linéaires. Voici pourquoi :\n",
    "\n",
    "- Pour le réseau de neurones \n",
    "    "
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 2 : Mise en oeuvre des modèles.\n",
    "\n",
    "# Arbre de décision \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.166163082645376\n",
      "11.166163082645376\n"
     ]
    }
   ],
   "source": [
    "# Arbre de décision\n",
    "\n",
    "# Calcul de l'entropie\n",
    "\n",
    "\"\"\"\n",
    "L'entropie est une mesure de l'incertitude associée à une variable aléatoire.\n",
    "\"\"\"\n",
    "\n",
    "def entropie(dataframe , attribut_cible):  \n",
    "    # Calcul de la probabilité de chaque classe\n",
    "    compte_classe = dataframe[attribut_cible].value_counts()\n",
    "    proba = compte_classe / compte_classe.sum() \n",
    "    # Calcul de l'entropie\n",
    "    entropie = - (proba * np.log2(proba+ np.finfo(float).eps)).sum() # éviter log2(0)\n",
    "    return entropie\n",
    "\n",
    "# Test de la fonction\n",
    "print(entropie(data, 'Attr_A'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "11.166163082646115\n",
    "11.166163082645376\n",
    "\n",
    "1.8608867211835993\n",
    "1.860886721183598"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voici l'implémentation en Python de l'algorithme décrit dans votre image\n",
    "\n",
    "def calculate_information_gain(data, attribute, target_attribute):\n",
    "    # Tri des données selon l'attribut\n",
    "    sorted_data = data.sort_values(by=attribute)\n",
    "    unique_values = sorted_data[attribute].unique()\n",
    "    \n",
    "    # Entropie initiale de l'ensemble de données\n",
    "    total_entropy = calculate_entropy(data[target_attribute])\n",
    "    \n",
    "    # Initialisation des variables pour capturer le meilleur gain et la meilleure valeur de split\n",
    "    best_gain = 0\n",
    "    best_split_value = None\n",
    "    best_partitions = None\n",
    "    \n",
    "    # Itération sur toutes les valeurs uniques de l'attribut, à l'exception de la dernière\n",
    "    for i in range(len(unique_values) - 1):\n",
    "        # Calcul de la valeur de split comme la moyenne de deux valeurs successives\n",
    "        split_value = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "        # Création de partitions basées sur la valeur de split\n",
    "        partition1 = data[data[attribute] < split_value]\n",
    "        partition2 = data[data[attribute] >= split_value]\n",
    "        \n",
    "        # Calcul de l'entropie pour chaque partition\n",
    "        entropy1 = calculate_entropy(partition1[target_attribute])\n",
    "        entropy2 = calculate_entropy(partition2[target_attribute])\n",
    "        \n",
    "        # Calcul du gain d'information\n",
    "        weight1 = len(partition1) / len(data)\n",
    "        weight2 = len(partition2) / len(data)\n",
    "        gain = total_entropy - (weight1 * entropy1 + weight2 * entropy2)\n",
    "        \n",
    "        # Si ce gain est meilleur que le précédent, le stocker ainsi que la valeur de split\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_split_value = split_value\n",
    "            best_partitions = (partition1, partition2)\n",
    "            \n",
    "    return attribute, best_gain, best_split_value, best_partitions\n",
    "\n",
    "# Utilisation de la fonction sur le dataframe\n",
    "# Notez que ceci doit être fait pour chaque attribut, ici on montre un exemple pour 'Attr_A'\n",
    "attribute, gain, split_value, partitions = calculate_information_gain(data, 'Attr_A', 'Class')\n",
    "print(f\"Best split for '{attribute}' is at value {split_value} with gain {gain}\")\n",
    "# Note: Cette fonction renvoie des informations pour un seul attribut. Vous devrez boucler sur tous les attributs pour les traiter tous."
   ]
=======
>>>>>>> 550e255dbe9f603d6297cfe010121c3e76411872
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
