{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Attr_A     Attr_B       Attr_C      Attr_D      Attr_E       Attr_F  \\\n",
      "0  12.478064  14.784992  1247.541877  100.962061   52.462177  1089.398211   \n",
      "1   8.264345   8.854181  1389.686814   99.526529   65.106526  1186.523399   \n",
      "2   9.875571  13.292442   779.077401  123.993772  104.699796  1201.722480   \n",
      "3   9.207661   9.346913   965.468523   89.176009  102.628284   743.913507   \n",
      "4   8.863842  12.542969  1096.386230  106.595385  131.813380   883.059615   \n",
      "\n",
      "      Attr_G     Attr_H     Attr_I      Attr_J       Attr_K     Attr_L  \\\n",
      "0  10.575834   8.375407  10.288159  110.746551   994.367610   9.069350   \n",
      "1   9.500485  10.088058   9.371983   78.210274   943.089589   9.988919   \n",
      "2   9.545266  14.266238   9.703551   86.252483  1082.989190  10.084217   \n",
      "3   9.777953  11.613946   8.912059   96.727873   812.800511   8.621781   \n",
      "4  10.092974  13.556029  11.649982   21.566576   971.083175  10.072271   \n",
      "\n",
      "        Attr_M      Attr_N  Class  \n",
      "0  1027.953917  109.672758      1  \n",
      "1  1120.317724   83.498764      3  \n",
      "2   970.953682   93.557046      2  \n",
      "3   947.207195  120.890054      3  \n",
      "4  1007.583900  149.511979      2  \n"
     ]
    }
   ],
   "source": [
    "# Exo 1 - Préparation des données \n",
    "\n",
    "# importation des données\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "# import du fichier\n",
    "data = pd.read_csv(\"synthetic.csv\")\n",
    "\n",
    "# Visualisation des données\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def neural_split_data(data, target_attribute):\n",
    "\n",
    "    # Séparez les données et les labels afin de pouvoir séparer en jeu d'entrainement\n",
    "    # de test et de validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[target_attribute]),\n",
    "                                                        data[target_attribute], test_size=0.15, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15/0.85,\n",
    "                                                    random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    \"\"\"Converts a vector of class indices into a one-hot encoded matrix.\"\"\"\n",
    "    y_array = np.array(y)  # Convert the pandas Series to a NumPy array if necessary\n",
    "    return np.eye(num_classes)[y_array.reshape(-1)]\n",
    "\n",
    "\n",
    "#print (X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.166163082645376\n"
     ]
    }
   ],
   "source": [
    "# Calcul de l'entropie\n",
    "\n",
    "\"\"\"\n",
    "L'entropie est une mesure de l'incertitude associée à une variable aléatoire.\n",
    "\"\"\"\n",
    "\n",
    "def entropie(dataframe , attribut_cible):  \n",
    "    # Calcul de la probabilité de chaque classe\n",
    "    compte_classe = dataframe[attribut_cible].value_counts()\n",
    "    #print(compte_classe)\n",
    "    proba = compte_classe / compte_classe.sum()\n",
    "    #print(proba) \n",
    "    # Calcul de l'entropie\n",
    "    entropie = - (proba * np.log2(proba+ np.finfo(float).eps)).sum() # éviter log2(0)\n",
    "    return entropie\n",
    "\n",
    "# Test de la fonction\n",
    "print(entropie(data, 'Attr_A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss:  0.010199795719758053\n",
      "Debugging Entropy Calculation:\n",
      "Calculated Entropy: 1.556656707462822\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# Cross Entropy function.\n",
    "def cross_entropy(y_pred, y_true):\n",
    " \n",
    "    # computing softmax values for predicted values\n",
    "    y_pred = softmax(y_pred)\n",
    "    loss = 0\n",
    "     \n",
    "    # Doing cross entropy Loss\n",
    "    for i in range(len(y_pred)):\n",
    " \n",
    "        # Here, the loss is computed using the\n",
    "        # above mathematical formulation.\n",
    "        loss = loss + (-1 * y_true[i]*np.log(y_pred[i]))\n",
    " \n",
    "    return loss\n",
    " \n",
    "# y_true: True Probability Distribution\n",
    "y_true = [1, 0, 0, 0, 0]\n",
    " \n",
    "# y_pred: Predicted values for each calss\n",
    "y_pred = [10, 5, 3, 1, 4]\n",
    "\n",
    "# Calling the cross_entropy function by passing\n",
    "# the suitable values\n",
    "cross_entropy_loss = cross_entropy(y_pred, y_true)\n",
    " \n",
    "print(\"Cross Entropy Loss: \", cross_entropy_loss)\n",
    "\n",
    "\n",
    "print(\"Debugging Entropy Calculation:\")\n",
    "debug_data = pd.DataFrame({\"Class\": [1, 1, 1, 0, 0, 2, 2]})\n",
    "print(f\"Calculated Entropy: {entropie(debug_data, 'Class')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1:\n",
      "Activations shape: (14, 4)\n",
      "Weights shape: (8, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (8,10) and (14,4) not aligned: 10 (dim 1) != 14 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/marcolyantoine/Desktop/ia/test.ipynb Cellule 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m neural_network \u001b[39m=\u001b[39m NeuralNetwork(architecture, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m \u001b[39m# Train the neural network using the training data\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m neural_network\u001b[39m.\u001b[39mtrain(X_train, y_train_encoded, epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m \u001b[39m# Evaluate the neural network using the validation data\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m validation_loss \u001b[39m=\u001b[39m neural_network\u001b[39m.\u001b[39mevaluate(X_val, y_val_encoded)\n",
      "\u001b[1;32m/Users/marcolyantoine/Desktop/ia/test.ipynb Cellule 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m X_batch \u001b[39m=\u001b[39m X[:, start_index:end_index]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m y_batch \u001b[39m=\u001b[39m y[:, start_index:end_index]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m activations, weighted_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_propagation(X_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m gradients \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_propagation(X_batch, y_batch, activations, weighted_inputs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_parameters(gradients, learning_rate)\n",
      "\u001b[1;32m/Users/marcolyantoine/Desktop/ia/test.ipynb Cellule 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mActivations shape: \u001b[39m\u001b[39m{\u001b[39;00mactivations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWeights shape: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[i]\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m weighted_input \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[i],activations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] ) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbiases[i]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m weighted_inputs\u001b[39m.\u001b[39mappend(weighted_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcolyantoine/Desktop/ia/test.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m activations\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_function(weighted_input))\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (8,10) and (14,4) not aligned: 10 (dim 1) != 14 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, architecture, activation='relu'):\n",
    "        self.architecture = architecture\n",
    "        self.activation = activation\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(42)\n",
    "        for i in range(1, len(self.architecture)):\n",
    "            if self.activation == 'relu':\n",
    "                # He initialization for ReLU\n",
    "                weight_matrix = np.random.randn(self.architecture[i], self.architecture[i - 1]) * np.sqrt(2. / self.architecture[i - 1])\n",
    "            else:\n",
    "                # Xavier initialization for other activations\n",
    "                weight_matrix = np.random.randn(self.architecture[i], self.architecture[i - 1]) * np.sqrt(1. / self.architecture[i - 1])\n",
    "            bias_vector = np.zeros((self.architecture[i], 1))\n",
    "            self.weights.append(weight_matrix)\n",
    "            self.biases.append(bias_vector)\n",
    "\n",
    "    def activation_function(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function. Choose 'relu' or 'tanh'.\")\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function for derivative. Choose 'relu' or 'tanh'.\")\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X.T]\n",
    "        weighted_inputs = []\n",
    "        for i in range(len(self.weights)):\n",
    "            if i == 0:\n",
    "                activations[-1]= activations[-1].T\n",
    "            print(f\"Layer {i}:\")\n",
    "            print(f\"Activations shape: {activations[-1].shape}\")\n",
    "            print(f\"Weights shape: {self.weights[i].shape}\")\n",
    "            weighted_input = np.dot(self.weights[i],activations[-1] ) + self.biases[i]\n",
    "            weighted_inputs.append(weighted_input)\n",
    "            activations.append(self.activation_function(weighted_input))\n",
    "        return activations, weighted_inputs\n",
    "    \n",
    "    def backward_propagation(self, X, y, activations, weighted_inputs):\n",
    "        m = X.shape[1]\n",
    "        delta = activations[-1] - y\n",
    "        dw = []\n",
    "        db = []\n",
    "        \n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            dw_i = np.dot(delta, activations[i].T) / m\n",
    "            db_i = np.sum(delta, axis=1, keepdims=True) / m\n",
    "            dw.insert(0, dw_i)\n",
    "            db.insert(0, db_i)\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = np.dot(self.weights[i].T, delta) * self.activation_derivative(weighted_inputs[i - 1])\n",
    "\n",
    "        return {'dw': dw, 'db': db}\n",
    "\n",
    "    # def backward_propagation(self, X, y, activations, weighted_inputs):\n",
    "    #     m = X.shape[1]\n",
    "    #     gradients = [np.zeros_like(w) for w in self.weights]\n",
    "    #     delta = activations[-1] - y\n",
    "    #     for i in range(m - 1, 0, -1):\n",
    "    #         gradients[i] = np.dot(delta, activations[i - 1].T) / m\n",
    "    #         delta = np.dot(self.weights[i].T, delta) * self.activation_derivative(weighted_inputs[i - 1])\n",
    "    #     gradients[0] = np.dot(delta, X.T) / m\n",
    "    #     return gradients\n",
    "    \n",
    "    # def update_parameters(self, gradients, learning_rate):\n",
    "    #     for i in range(len(self.weights)):\n",
    "    #         self.weights[i] -= learning_rate * gradients[i]\n",
    "    #         self.biases[i] -= learning_rate * gradients[i]\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * gradients['dw'][i]\n",
    "            self.biases[i] -= learning_rate * gradients['db'][i]\n",
    "\n",
    "    def train(self, X, y, epochs=1000, learning_rate=0.01, batch_size=4, patience=4):\n",
    "        X = np.array(X).T  # Ensuring X is a NumPy array, transposed to match shape expectations\n",
    "        y = np.array(y).T  # Ensuring y is transposed to match (num_classes, num_samples)\n",
    "        \n",
    "        # Setup for early stopping\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        # Batch training implementation\n",
    "        num_samples = X.shape[1]\n",
    "        num_batches = num_samples // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_index in range(num_batches):\n",
    "                start_index = batch_index * batch_size\n",
    "                end_index = start_index + batch_size\n",
    "                X_batch = X[:, start_index:end_index]\n",
    "                y_batch = y[:, start_index:end_index]\n",
    "\n",
    "                activations, weighted_inputs = self.forward_propagation(X_batch)\n",
    "                gradients = self.backward_propagation(X_batch, y_batch, activations, weighted_inputs)\n",
    "                self.update_parameters(gradients, learning_rate)\n",
    "\n",
    "            # Evaluate on validation set after each epoch\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_loss = self.evaluate(X_val, y_val)\n",
    "                print(f\"Epoch {epoch}, Validation Loss: {val_loss}\")\n",
    "\n",
    "                # Check for early stopping\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "            # Optional: print training loss during training\n",
    "            if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "                train_loss = self.evaluate(X, y)  # Evaluate on the entire training set\n",
    "                print(f\"Epoch {epoch}, Training Loss: {train_loss}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).T  # Ensure y is transposed\n",
    "        predictions = self.predict(X)\n",
    "        loss = cross_entropy(predictions, y)\n",
    "        return loss\n",
    "    \n",
    "    # def train(self, X, y, epochs=1000, learning_rate=0.01):\n",
    "    #     X = np.array(X) \n",
    "    #     y = np.array(y)\n",
    "    #     for epoch in range(epochs):\n",
    "    #         activations, weighted_inputs = self.forward_propagation(X)\n",
    "    #         gradients = self.backward_propagation(X, y, activations, weighted_inputs)\n",
    "    #         self.update_parameters(gradients, learning_rate)\n",
    "    #         if epoch % 100 == 0:\n",
    "    #             loss = cross_entropy(activations[-1], y)\n",
    "    #             print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    # def predict(self, X):\n",
    "    #     activations, _ = self.forward_propagation(X)\n",
    "    #     return activations[-1]\n",
    "    \n",
    "    # def evaluate(self, X, y):\n",
    "    #     predictions = self.predict(X)\n",
    "    #     loss = cross_entropy(predictions, y)\n",
    "    #     return loss\n",
    "    \n",
    "    def print_parameters(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            print(f\"Layer {i + 1}:\")\n",
    "            print(\"Weights:\")\n",
    "            print(self.weights[i])\n",
    "            print(\"Biases:\")\n",
    "            print(self.biases[i])\n",
    "\n",
    "\n",
    "# Define the target attribute\n",
    "target_attribute = \"Class\"\n",
    "\n",
    "# Split the data into training, validation, and testing sets using the neural_split_data function\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = neural_split_data(data, target_attribute)\n",
    "\n",
    "\n",
    "# Define the architecture of the neural network\n",
    "architecture = [14,10,8,6, 4]\n",
    "\n",
    "num_classes = 4  # Adjust based on your specific dataset\n",
    "y_train_encoded = one_hot_encode(y_train, num_classes)\n",
    "y_val_encoded = one_hot_encode(y_val, num_classes)\n",
    "y_test_encoded = one_hot_encode(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Instantiate the NeuralNetwork class\n",
    "neural_network = NeuralNetwork(architecture, activation='relu')\n",
    "\n",
    "# Train the neural network using the training data\n",
    "neural_network.train(X_train, y_train_encoded, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "# Evaluate the neural network using the validation data\n",
    "validation_loss = neural_network.evaluate(X_val, y_val_encoded)\n",
    "print(f\"Validation Loss: {validation_loss}\")\n",
    "\n",
    "# Print the parameters of the neural network\n",
    "neural_network.print_parameters()\n",
    "\n",
    "# Evaluate the neural network using the testing data\n",
    "test_loss = neural_network.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Predict the target attribute for the testing data\n",
    "predictions = neural_network.predict(X_test)\n",
    "print(predictions)\n",
    "\n",
    "# # Evaluate the model using accuracy, precision, recall, and F1-score\n",
    "# accuracy = accuracy_score(y_test, predictions)\n",
    "# eval_accuracy = evaluation_metric.accuracy_score(y_test, predictions)\n",
    "\n",
    "# precision = precision_score(y_test, predictions, average='weighted')\n",
    "# eval_precision = evaluation_metric.precision_score(y_test, predictions, positive_label=1)\n",
    "\n",
    "# recall = recall_score(y_test, predictions, average='weighted')\n",
    "# eval_recall = evaluation_metric.recall_score(y_test, predictions, positive_label=1)\n",
    "\n",
    "# f1 = f1_score(y_test, predictions, average='weighted')\n",
    "# eval_f1 = evaluation_metric.f1_score(y_test, predictions, positive_label=1)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")\n",
    "# print(f\"Eval Accuracy: {eval_accuracy:.2f}\")\n",
    "\n",
    "# print(f\"Precision: {precision:.2f}\")\n",
    "# print(f\"Eval Precision: {eval_precision:.2f}\")\n",
    "\n",
    "# print(f\"Recall: {recall:.2f}\")\n",
    "# print(f\"Eval Recall: {eval_recall:.2f}\")\n",
    "\n",
    "# print(f\"F1-score: {f1:.2f}\")\n",
    "# print(f\"Eval F1-score: {eval_f1:.2f}\")  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
